[
  {
    "objectID": "case_study_1.html",
    "href": "case_study_1.html",
    "title": "Case Analysis 1: Olympics",
    "section": "",
    "text": "TidyTuesday Reference\nOriginal Data Source\nObjective: Usually it is the Olympic athletes that receive the fame. However, at the end of the day they each represent their country with pride. Let’s find out which nations are represented the most in the top 10 most successful athlete list (measured by medal count). So, which countries have produced the best talents?\n\n\n\nTop 10 Athletes by Total Medals (USSR/Russia Combined)\n\n\nName\nTeam\nTotal Medals\n\n\n\n\nMichael Fred Phelps, II\nUnited States\n28\n\n\nLarysa Semenivna Latynina (Diriy-)\nUSSR/Russia\n18\n\n\nNikolay Yefimovich Andrianov\nUSSR/Russia\n15\n\n\nBorys Anfiyanovych Shakhlin\nUSSR/Russia\n13\n\n\nEdoardo Mangiarotti\nItaly\n13\n\n\nOle Einar Bjrndalen\nNorway\n13\n\n\nTakashi Ono\nJapan\n13\n\n\nAleksey Yuryevich Nemov\nUSSR/Russia\n12\n\n\nDara Grace Torres (-Hoffman, -Minas)\nUnited States\n12\n\n\nJennifer Elisabeth \"Jenny\" Thompson (-Cumpelik)\nUnited States\n12\n\n\n\n\n\n\n\n\nTop Nations by Number of Top Athletes (USSR/Russia Combined)\n\n\nTeam\nNumber of Athletes\n\n\n\n\nUSSR/Russia\n4\n\n\nUnited States\n3\n\n\nItaly\n1\n\n\nJapan\n1\n\n\nNorway\n1\n\n\n\n\n\n\n\nAs for most successful athletes, Michael Phelps by far has won the most medals (28). His success if unprecedented, with the second most decorated athlete having a whopping 10 fewer medals than Phelps.\n\n\n\n\n\n\n\n\n\nRussia (formerly USSR) produced the highest number of most successful athletes. Russia/USSR and the US usually rank top 2 for medal counts. It is actually surprising to find China not represented in the list, given the recent success of China in Olympics."
  },
  {
    "objectID": "case_study_3.html",
    "href": "case_study_3.html",
    "title": "Case Analysis 3: Netflix Movies",
    "section": "",
    "text": "TidyTuesday Reference\nOriginal Data Source\nObjective: Analyze some interesting trends in Netflix movies as of 2019\nThis dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search engine.\nIn 2018, Flixable released an interesting report which shows that the number of TV shows on Netflix has nearly tripled since 2010. The streaming service’s number of movies has decreased by more than 2,000 titles since 2010, while its number of TV shows has nearly tripled. It will be interesting to explore what other insights can be obtained from the same dataset.\nIntegrating this dataset with other external datasets such as IMDB ratings, rotten tomatoes can also provide many interesting findings.\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-04-20')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 17)\n\nnetflix &lt;- tuesdata$netflix\n\nlibrary(tidyverse)\n\n\n\n\nactor_count &lt;- netflix |&gt;\n  filter(type == \"Movie\", !is.na(cast)) |&gt; \n  separate_rows(cast, sep = \",\\\\s*\") |&gt; \n  count(cast, sort = TRUE)\n\ntop_5_actors &lt;- actor_count |&gt;\n  slice_head(n = 5) \n\n\nggplot(top_5_actors, aes(x = reorder(cast, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +  \n  labs(\n    title = \"Top 5 Actors Appearing in Netflix Titles\",\n    x = \"Actor\",\n    y = \"Number of Appearances\"\n  ) +\n  theme_minimal() \n\n\n\n\n\n\n\n\nInteresting, I actually do not know any of these actors/actresses. Upon some research, I have found that all of them are Indian actors/actresses. These actors/actresses were very active in their respective careers.\nAs a side analysis, let’s find out which countries produced the most number of movies?\n\ncountry_count_movies &lt;- netflix |&gt;\n  filter(type == \"Movie\", !is.na(country)) |&gt;  \n  separate_rows(country, sep = \",\\\\s*\") |&gt;  \n  count(country, sort = TRUE)  \n\ntop_5_countries &lt;- country_count_movies |&gt;\n  slice_head(n = 5)  \n\n\nggplot(top_5_countries, aes(x = reorder(country, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +  \n  labs(\n    title = \"Top 5 Countries Producing the Most Netflix Movies\",\n    x = \"Country\",\n    y = \"Number of Movies\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nUnsurprisingly, the US has produced the most Netflix movies. This makes sense given that Hollywood is the global hub of film production. The US has produced more than double the amount of movies than second place India and more than the movie productions of the next four countries combined. With the growing size of Bollywood, India comes in for second place. This may speak to the high number of Indian actors/actresses in the first study.\n\n\n\nWe have excluded articles, pronouns, prepositions and any common stop words.\n\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(stringr)\n\n# A list of words we want to exclude\nstop_words &lt;- c(\"a\", \"an\", \"the\", \"and\", \"in\",\n                \"on\", \"of\", \"with\", \"for\", \"to\",\n                \"he\", \"she\", \"my\", \"it\", \"him\",\n                \"her\", \"above\", \"below\", \"top\",\n                \"bottom\", \"between\", \"front\", \"back\",\n                \"beneath\", \"they\", \"me\", \"you\", \"them\",\n                \"us\", \"we\", \"that\", \"this\", \"these\",\n                \"those\", \"I\", \"from\", \"i\", \"at\", \"\")\n\n# Step 1: Filter and preprocess the titles\nword_count &lt;- netflix |&gt;\n  subset(type == \"Movie\" & !is.na(title)) |&gt;\n  transform(clean_title = str_remove_all(title, \"[^A-Za-z\\\\s]\")) |&gt;\n  transform(clean_title = tolower(clean_title)) |&gt;\n  {\\(df) data.frame(clean_title = unlist(strsplit(df$clean_title, \"\\\\s+\")))}() |&gt;\n  subset(!clean_title %in% stop_words) |&gt;\n  table() |&gt;\n  as.data.frame()\n\n# Step 2: Sort and rename columns\nword_count &lt;- word_count[order(-word_count$Freq), ]\ncolnames(word_count) &lt;- c(\"Word\", \"Frequency\")\n\n# Step 3: Display top 10 words as a formatted table\nword_count |&gt;\n  head(10) |&gt;\n  kable(\n    caption = \"&lt;strong style='color:black; font-size:16px;'&gt;Top 10 Most Frequent Words in Netflix Movie Titles&lt;/strong&gt;\",\n    col.names = c(\"Word\", \"Frequency\"),\n    row.names = FALSE,  # Suppress row numbers\n    align = c(\"l\", \"c\"),\n    escape = FALSE\n  ) |&gt;\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),\n    full_width = FALSE,\n    position = \"center\"\n  )\n\n\nTop 10 Most Frequent Words in Netflix Movie Titles\n\n\nWord\nFrequency\n\n\n\n\nlove\n91\n\n\nchristmas\n72\n\n\nmovie\n60\n\n\nman\n58\n\n\nstory\n54\n\n\nlife\n44\n\n\nworld\n42\n\n\nlittle\n39\n\n\nlive\n38\n\n\none\n38\n\n\n\n\n\n\n\nLove and Christmas are the top two most common words. These results do makes sense as these are popular topics.\n\n\n\nThe most commercially successful filmmaker in the history of Hollywood is Steven Spielberg. Steven Spielberg is the creator of many of the most successful franchises and stand-alone films. They include the Indiana Jones series, the Jurassic Park series, Saving Private Ryan, and Jaws. Let’s figure out which of his movies are on Netflix.\n\nspielberg_movies &lt;- netflix |&gt;\n  filter(type == \"Movie\", str_detect(director, \"Steven Spielberg\"))\n\nspielberg_movies |&gt;\n  select(title) |&gt;  # Select only the title column\n  rename(\"Title\" = title) |&gt;  # Rename the column for clarity\n  kable(\n    caption = \"&lt;strong style='color:black; font-size:16px;'&gt;Steven Spielberg Movies Available on Netflix&lt;/strong&gt;\",\n    col.names = c(\"Title\"),\n    align = \"l\",\n    escape = FALSE \n  ) |&gt;\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),\n    full_width = FALSE,\n    position = \"center\"\n  )\n\n\nSteven Spielberg Movies Available on Netflix\n\n\nTitle\n\n\n\n\nCatch Me If You Can\n\n\nHook\n\n\nIndiana Jones and the Kingdom of the Crystal Skull\n\n\nIndiana Jones and the Last Crusade\n\n\nIndiana Jones and the Raiders of the Lost Ark\n\n\nIndiana Jones and the Temple of Doom\n\n\nLincoln\n\n\nSchindler's List\n\n\nThe Adventures of Tintin\n\n\nWar Horse\n\n\n\n\n\n\n\nSurprising, only 10 are available on Netflix. Given sheer amount of Spielberg movies we know of, this a very small number."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Taehwan (Jack) Kim",
    "section": "",
    "text": "Greetings! I’m Jack. Junior and Economics major at Pomona College. Formerly of Class of 2024, finally back at school after serving in the Republic of Korea Army. My interest lies in the world of finance and value investing. Love to play soccer (lifelong Arsenal fan) and love to share my military endeavors. This is a space to test my analytical skillset."
  },
  {
    "objectID": "Project_2.html",
    "href": "Project_2.html",
    "title": "Project_2",
    "section": "",
    "text": "TidyTuesday Reference\nOriginal Data Source\nThis dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search engine.\nIn 2018, they released an interesting report which shows that the number of TV shows on Netflix has nearly tripled since 2010. The streaming service’s number of movies has decreased by more than 2,000 titles since 2010, while its number of TV shows has nearly tripled. It will be interesting to explore what all other insights can be obtained from the same dataset.\nIntegrating this dataset with other external datasets such as IMDB ratings, rotten tomatoes can also provide many interesting findings.\n\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-04-20')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 17)\n\nnetflix &lt;- tuesdata$netflix\n\nlibrary(tidyverse)\n\n\nWho are the actors with the most movie appearances? Let’s find the top 5 actors by the number of appearances.\n\n\nactor_count &lt;- netflix |&gt;\n  filter(type == \"Movie\", !is.na(cast)) |&gt; \n  separate_rows(cast, sep = \",\\\\s*\") |&gt; \n  count(cast, sort = TRUE)\n\ntop_5_actors &lt;- actor_count |&gt;\n  slice_head(n = 5) \n\n\nggplot(top_5_actors, aes(x = reorder(cast, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +  \n  labs(\n    title = \"Top 5 Actors Appearing in Netflix Titles\",\n    x = \"Actor\",\n    y = \"Number of Appearances\"\n  ) +\n  theme_minimal() \n\n\n\n\n\n\n\n\nInteresting, I actually do not know any of these actors/actresses. Upon some research, I have found that all of them are Indian actors/actresses.\nJust out of curiosity, let’s find out which countries produced the most number of movies?\n\ncountry_count_movies &lt;- netflix |&gt;\n  filter(type == \"Movie\", !is.na(country)) |&gt;  \n  separate_rows(country, sep = \",\\\\s*\") |&gt;  \n  count(country, sort = TRUE)  \n\ntop_5_countries &lt;- country_count_movies |&gt;\n  slice_head(n = 5)  \n\n\nggplot(top_5_countries, aes(x = reorder(country, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +  \n  labs(\n    title = \"Top 5 Countries Producing the Most Netflix Movies\",\n    x = \"Country\",\n    y = \"Number of Movies\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nUnsurprisingly, the US has produced the most Netflix movies.This makes sense given that Hollywood is the global hub of film production. With the growing size of Bollywood, India comes in for second place.\n\nMost popular words in movie titles:\n\nWe have excluded articles, pronouns, prepositions and any common stop words.\n\n# A list of words we want to exclude\nstop_words &lt;- c(\"a\", \"an\", \"the\", \"and\", \"in\", \"on\", \"of\", \"with\", \"for\", \"to\", \"he\", \"she\", \"my\", \"it\", \"him\", \"her\", \"above\", \"below\", \"top\", \"bottom\", \"between\", \"front\", \"back\", \"beneath\", \"they\", \"me\", \"you\", \"them\", \"us\", \"we\", \"that\", \"this\", \"these\", \"those\", \"I\", \"from\", \"i\", \"at\", \"\")\n\nword_count &lt;- netflix |&gt;\n  filter(type == \"Movie\", !is.na(title)) |&gt;\n  mutate(clean_title = str_remove_all(title, \"[^A-Za-z\\\\s]\")) |&gt;\n  mutate(clean_title = str_to_lower(clean_title)) |&gt;\n  separate_rows(clean_title, sep = \"\\\\s+\") |&gt; \n  filter(!clean_title %in% stop_words) |&gt; \n  count(clean_title, sort = TRUE) \n\nhead(word_count, 10)\n\n# A tibble: 10 × 2\n   clean_title     n\n   &lt;chr&gt;       &lt;int&gt;\n 1 love           91\n 2 christmas      72\n 3 movie          60\n 4 man            58\n 5 story          54\n 6 life           44\n 7 world          42\n 8 little         39\n 9 live           38\n10 one            38\n\n\nLove and Christmas are the top two most common words. These results do makes sense as these are pretty popular topics.\n\nThe most commercially successful filmmaker in the history of Hollywood is Steven Spielburg. Let’s figure out which of his movies are on Netflix.\n\n\nspielberg_movies &lt;- netflix |&gt;\n  filter(type == \"Movie\", str_detect(director, \"Steven Spielberg\"))\n\nhead(spielberg_movies$title)\n\n[1] \"Catch Me If You Can\"                               \n[2] \"Hook\"                                              \n[3] \"Indiana Jones and the Kingdom of the Crystal Skull\"\n[4] \"Indiana Jones and the Last Crusade\"                \n[5] \"Indiana Jones and the Raiders of the Lost Ark\"     \n[6] \"Indiana Jones and the Temple of Doom\"              \n\n\nSurprising, only 6 are available on Netflix. Given sheer amount of Spielberg movies we know of, this a very small number.\n\nFollowing Covid-19, less people are going to the movies. At the same time over-the-top streaming services have become more influential. Nowadays, people are wondering “when is my favorite movie going to be on Netflix?”. So let’s find out, on average, how long does it take for a movie to be added on Netflix.\n\nTwo caveats: - We have exact dates for the when the movie was added, but only the year data is available for when it was released. Hence, we will just find the number of years. - Netflix started streaming services in 2007. Any movie produced before 2007 will be excluded because that just automatically increases the average.\n\nnetflix &lt;- netflix |&gt;\n  mutate(\n    date_added = mdy(date_added),  \n    release_year = as.numeric(release_year)  \n  )\n\nnetflix &lt;- netflix |&gt;\n  mutate(added_year = year(date_added))  \n\nnetflix &lt;- netflix |&gt;\n  mutate(time_diff_years = added_year - release_year)  \n\nnetflix_era &lt;- netflix |&gt;\n  filter(!is.na(time_diff_years) & time_diff_years &gt;= 0 & release_year &gt;= 2007) \n\naverage_time_diff &lt;- netflix_era |&gt;\n  summarize(average_diff = mean(time_diff_years, na.rm = TRUE))\n\naverage_time_diff\n\n# A tibble: 1 × 1\n  average_diff\n         &lt;dbl&gt;\n1         2.14\n\n\nOn average, it takes 2.14 years for a movie to be added on Netflix after being released in the theaters."
  },
  {
    "objectID": "Project_3.html",
    "href": "Project_3.html",
    "title": "Project 3: NYC Flights Analysis",
    "section": "",
    "text": "Objective: In this analysis, I will investigate whether there is a significant difference in departure delays between morning and evening flights using data from the nycflights13 package. To do this, I will perform a permutation test, where I will shuffle the time-of-day labels (morning/evening) and repeatedly calculate the difference in mean departure delays. This will help simulate the null hypothesis, which assumes that the time of day has no statistical significance on departure delays. By comparing the observed difference in delays to this simulated distribution, I will assess the statistical significance of the observed difference.\n[Null Hypothesis] There is no difference in the mean departure delays between morning flights and evening flights.\n\nlibrary(nycflights13)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(DT)\n\n\nflights &lt;- nycflights13::flights |&gt;\n  filter(!is.na(dep_delay)) |&gt;\n  mutate(time_of_day = case_when(\n    hour &gt;= 5 & hour &lt; 11 ~ \"morning\",\n    hour &gt;= 17 & hour &lt; 23 ~ \"evening\",\n    TRUE ~ \"other\"\n  )) |&gt;\n  filter(time_of_day != \"other\")  # Exclude flights that are not in the morning or evening\n\n\n# Display the first 5 rows as an interactive table\ndatatable(\n  flights[1:5, ],  # Select the first 5 rows\n  options = list(\n    pageLength = 5,      # Display 5 rows per page\n    dom = 't',           # Show only the table (no search box, etc.)\n    scrollX = TRUE       # Enable horizontal scrolling\n  ),\n  caption = \"First 5 Rows of Flights Data\"\n)\n\n\n\n\n\nWe are using the nycflights13 data which contains the flight information in New York airports in 2013. Most importantly, the data shows the departure delay time (in minutes). We will first define morning and evening flights. Morning flights are flights departing between 5:00am and 11:00am. Evening flights are flights departing from 5:00pm and 11:00pm (ex. First row, dep_time 517 means departure time is at 5:17am). We removed the flights that are not within these time ranges.\nBelow we have the average delayed time for morning and evening flights.\n\nlibrary(knitr)\nlibrary(kableExtra)\n\nmean_delays &lt;- flights |&gt;\n  group_by(time_of_day) |&gt;\n  summarize(mean_delay = mean(dep_delay, na.rm = TRUE))\n\nmean_delays |&gt;\n  kable(\n    caption = \"Mean Departure Delay by Time of Day\",\n    col.names = c(\"Time of Day\", \"Mean Delay (minutes)\"),\n    align = c(\"l\", \"c\")\n  ) |&gt;\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),\n    full_width = FALSE,\n    position = \"center\"\n  )\n\n\nMean Departure Delay by Time of Day\n\n\nTime of Day\nMean Delay (minutes)\n\n\n\n\nevening\n22.729753\n\n\nmorning\n3.492098\n\n\n\n\n\n\n\nThen we calculate the observed differences.\n\nobs_diff &lt;- flights |&gt;\n  group_by(time_of_day) |&gt;\n  summarize(mean_delay = mean(dep_delay)) |&gt;\n  summarize(diff = diff(mean_delay)) |&gt;\n  pull(diff)\n\n\n\nObserved Difference in Mean Delays (Evening - Morning): -19.23765 minutes\n\n\n[Permutation Test] We’ll shuffle the time labels (morning or evening) 1000 times and calculate the difference in mean delays for each permutation.\n\nset.seed(47)\n\nnull_dist &lt;- map_dbl(1:1000, ~ {\n  flights |&gt; \n    mutate(shuffled_time = sample(time_of_day)) |&gt; \n    group_by(shuffled_time) |&gt; \n    summarize(mean_delay = mean(dep_delay), .groups = \"drop\") |&gt; \n    summarize(diff = diff(mean_delay)) |&gt; \n    pull(diff)\n})\n\nLet’s plot the null distribution with observed difference\n\nggplot(data.frame(null_dist), aes(x = null_dist)) +\n  geom_histogram(binwidth = 0.5, fill = \"lightblue\", color = \"black\") +\n  geom_vline(xintercept = obs_diff, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Permutation Test: Morning vs Evening Delays\",\n    x = \"Difference in Mean Delays\",\n    y = \"Frequency\"\n  )\n\n\n\n\n\n\n\n\n\n# Calculate p-value\np_value &lt;- mean(abs(null_dist) &gt;= abs(obs_diff))\n\n\n\nP-value: 0 \n\n\nResults\nThe red dashed line represents the observed difference in mean delays between morning and evening flights. The histogram represents the differences in mean delays generated under the null hypothesis (shuffling the time labels).\nA p-value of 0 means that none of the 1000 permutations generated a difference in mean delays as extreme (or more extreme) as the observed difference. This indicates that the observed difference is highly unlikely to have occurred by chance if the null hypothesis were true.\nSince the p-value is 0, this provides strong evidence against the null hypothesis. You would reject the null hypothesis and conclude that the time of day (morning vs. evening) may be correlated to departure delays.\nThe visualization supports this as the red line falls outside the range of the simulated null distribution values, highlighting that the observed difference is not consistent with the null hypothesis assumption.\nGreater Insights\nSo how can we interpret these results in a broader context? The current analysis is based on flights departing from NYC airports (JFK, LGA, EWR) in 2013 and data acquired under the assumption that morning and evening flights do not have missing delay information. This means the conclusion of a significant difference in mean delays applies only to NYC flights in 2013.\nFor potential larger populations, we could generalize the results to other years in NYC. Similar patterns of delays (morning flights being less delayed than evening flights) might hold for other years if no major changes occurred in scheduling, infrastructure, or air traffic. It may also be possible to generalize to similar metropolitan hubs. NYC is one of the busiest air travel hubs in the US, so patterns observed in NYC might apply to other large cities with comparable air traffic and flight schedules, such as Chicago (ORD, MDW), Los Angeles (LAX), or Atlanta (ATL). However, generalization should be done cautiously and supported by additional evidence. Moreover, there may be limitations to generalization in the sense that regional factors may come into play. NYC has unique air traffic patterns due to its size, location, and volume of international and domestic flights. Other cities may not share these characteristics."
  },
  {
    "objectID": "case_study_2.html",
    "href": "case_study_2.html",
    "title": "Case Analysis 2: Premier League 2021-2022",
    "section": "",
    "text": "TidyTuesday Reference\nOriginal Data Source\nObjective: Which referees gave out the most yellow cards to Arsenal during the 2021-2022 season?\nRefereeing decisions play huge outcome in a football match. In fact, fans and analysts alike do pay enormous attention to the designated referee for each fixture. As a lifelong Arsenal fan, I wanted to find out which referees tend to be harsher on Arsenal.\n\n\n\n\n\n\n\n\n\n\n\nThe bar chart represents the total number of yellow cards each referee has given to Arsenal. Craig Pawson handed out the most yellow cards to Arsenal, with 9 cards given.\n\n\n\n\n\n\n\n\n\n\n\n\nThis study offers insight into if Arsenal experienced home advantage in referee decisions. Many fans assume that referees tend to be more lenient toward home teams. This may manifest the most in referee decisions for making disciplinary actions. The bar chart shows the number of yellow cards Arsenal received depending on match venue.\nInterestingly, Arsenal received 29 yellow cards at away matches and 31 in home matches. This goes against common logic. Although not conclusive, home advantage did not work in Arsenal’s favor."
  },
  {
    "objectID": "Project_4.html",
    "href": "Project_4.html",
    "title": "Project 4: SQL",
    "section": "",
    "text": "Wideband acoustic immittance (WAI) Analysis\nOriginal Data Source\nBackground: The WAI database is a comprehensive online database for normative adult WAI measurements. This database is designed to facilitate data sharing and analysis among researchers in the field of audiology. As of July 1, 2019, the database encompasses measurements from 12 peer-reviewed studies, totaling 640 subjects and 914 normal middle ears. This results in 286,774 data points across various frequencies. The establishment of this WAI database represents a significant advancement in audiological research, offering a centralized resource for normative data and promoting collaborative efforts in the study of middle-ear pathologies.\nThe objective is to recreate Figure 1, which represents mean absorbance data from select 12 publications in the Wideband Acoustic Immittance (WAI) database, showing how absorbance varies with frequency across different studies.\nStarter Code to establish connection to the WAI database.\n\nlibrary(DBI)\nlibrary(RMariaDB)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ncon_wai &lt;- dbConnect(\n  MariaDB(),\n  host = \"scidb.smith.edu\",\n  user = \"waiuser\",\n  password = \"smith_waiDB\",\n  dbname = \"wai\"\n)\n\nMeasurements &lt;- tbl(con_wai, \"Measurements\")\nPI_Info &lt;- tbl(con_wai, \"PI_Info\")\nSubjects &lt;- tbl(con_wai, \"Subjects\")\n\nThis SQL code below is for processing and aggregating data from the WAI database. More specifically, this combines data from two tables, Measurements and PI_Info, to calculate and aggregate absorbance values for specific studies and prepare data for visualization.\nWe are: 1. Calculate Mean Absorbance: for each unique combination of study, instrument, frequency, and year. 2. Generates Legend Labels: constructs a label for visualization 3. Filters Data: restricts the query to 12 specific studies of interest. 4. Groups Data: organizes results by study, frequency, instrument, and year.\n\n\nSELECT \n    Measurements.Identifier, \n    PI_Info.AuthorsShortList, \n    Measurements.Instrument, \n    Measurements.Frequency, \n    AVG(Measurements.Absorbance) AS MeanAbsorbance, \n    CONCAT(\n        PI_Info.AuthorsShortList, ' (', PI_Info.Year, ') N=',\n        COUNT(DISTINCT CONCAT(Measurements.SubjectNumber, Measurements.Ear))\n    ) AS Legend_Label\nFROM Measurements\nJOIN PI_Info \n    ON Measurements.Identifier = PI_Info.Identifier\nWHERE Measurements.Identifier IN (\n    'Abur_2014', 'Feeney_2017', 'Groon_2015', 'Lewis_2015', \n    'Liu_2008', 'Rosowski_2012', 'Shahnaz_2006', 'Shaver_2013', \n    'Sun_2016', 'Voss_1994', 'Voss_2010', 'Werner_2010'\n)\nGROUP BY \n    Measurements.Identifier, \n    Measurements.Instrument, \n    PI_Info.AuthorsShortList, \n    Measurements.Frequency, \n    PI_Info.Year;\n\nData Visualization:\n\ndata$Frequency &lt;- as.numeric(data$Frequency)\n\n\ndata &lt;- data %&gt;%\n  filter(Frequency &gt;= 200)\n\n# Create the plot\nggplot(data, aes(x = Frequency, y = MeanAbsorbance, color = Legend_Label)) +\n  geom_line(size = 0.8) +\n  labs(\n    title = \"Mean absorbance from publications in WAI database\",\n    x = \"Frequency (Hz)\",\n    y = \"Mean Absorbance\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(\n    trans = \"log10\",\n    breaks = c(200, 400, 600, 800, 1000, 2000, 4000, 6000, 8000),\n    labels = c(\"200\", \"400\", \"600\", \"800\", \"1000\", \"2000\", \"4000\", \"6000\", \"8000\"),\n    limits = c(200, 8000)\n  ) +\n  scale_y_continuous(\n    limits = c(0, 1),\n    expand = c(0, 0)\n  )\n\n\n\n\n\n\n\n\nAbove is the a replicate of Figure 1. Y-axis (Mean Absorbance) represents the average proportion of sound energy absorbed by the middle ear at various frequencies. Higher values indicate greater absorption, while lower values suggest more reflection. X-axis (Frequency in Hz) represents the sound frequency (logarithmic scale) ranging from 200 Hz to 8000 Hz.\nEach line corresponds to a specific study in the WAI database, identified by the first author’s name, publication year, and the number of participants.\nIn essence, the graph provides a comparative view of absorbance data from multiple studies. It highlights similarities and differences in how different populations and systems respond to sound frequencies.\nMost studies show increasing absorbance from 200 Hz to approximately 1000-2000 Hz, peaking, and then gradually decreasing at higher frequencies. Variability in the data such as differences in peak values and slopes likely reflects variations in study populations, equipment, or methodologies.\nDeeper Dive into Feeney et al. (2017)\nI decided to choose the most recent study among the 12 selected publications, which was Feeney et al. (2017). This contains various grouping variables such as age, sex, and race/ethnicity. I chose Sex as the grouping variable because it is a common demographic factor in audiological studies and is likely to show differences in middle-ear characteristics.\nThe below SQL query takes on a similar process as that of recreating Figure 1. In this case, the Identifier would be the Feeney_2017, instead of all the 12 studies.\n\n\nSELECT \n    Measurements.Frequency, \n    Subjects.Sex,\n    AVG(Measurements.Absorbance) AS MeanAbsorbance\nFROM Measurements\nJOIN Subjects\n    ON Measurements.SubjectNumber = Subjects.SubjectNumber\nWHERE Measurements.Identifier = 'Feeney_2017'\nGROUP BY \n    Measurements.Frequency, \n    Subjects.Sex\nORDER BY Measurements.Frequency;\n\nData Visualization:\nI maintained a similar format for the x and y axis. The X-axis represents frequency (logarithmic scale), the Y-axis shows mean absorbance, and lines differentiate groups by sex (Male, Female, Unknown).\n\ndata$Frequency &lt;- as.numeric(data$Frequency)\n\nggplot(data, aes(x = Frequency, y = MeanAbsorbance, color = Sex)) +\n  geom_line(size = 0.8) +\n  labs(\n    title = \"Mean Absorbance by Sex Across Frequencies (Feeney et al., 2017)\",\n    x = \"Frequency (Hz)\",\n    y = \"Mean Absorbance\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(\n    trans = \"log10\",\n    breaks = c(200, 400, 600, 800, 1000, 2000, 4000, 6000, 8000),\n    labels = c(\"200\", \"400\", \"600\", \"800\", \"1000\", \"2000\", \"4000\", \"6000\", \"8000\"),\n    limits = c(200, 8000)\n  ) +\n  scale_y_continuous(\n    limits = c(0, 1),\n    expand = c(0, 0)\n  )\n\n\n\n\n\n\n\n\nThis graph illustrates how middle-ear absorbance varies across frequencies for male, female, and unknown sex groups in the Feeney et al. (2017) study. While the general trend of absorbance is consistent across groups, slight variations at the extremes of the frequency range (low and high) may warrant further exploration. The similarity across sexes suggests that sex is not a significant factor influencing WAI in this study.\nSome key observations: 1. Absorbance increases as frequency rises from 200 Hz to around 1000-2000 Hz, peaks, and then decreases for higher frequencies. All groups show peak absorbance in the range of 1000-4000 Hz, a typical finding in WAI studies, as this range represents optimal middle-ear energy absorption. 2. The absorbance patterns for male and female groups are very similar across frequencies, suggesting that sex may have minimal impact on WAI results in this study. The “Unknown” group also follows a similar trajectory, possibly due to overlapping populations. 3. Slight differences can be seen at lower frequencies (200-400 Hz) and higher frequencies (&gt;6000 Hz), but these differences are marginal.\nBest practice purposes…\n\ndbDisconnect(con_wai)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Taehwan (Jack) Kim | 김태환",
    "section": "",
    "text": "My life so far…"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Taehwan (Jack) Kim | 김태환",
    "section": "Education",
    "text": "Education\nPomona College - Claremont, California, USA\n\nMajor in Economics and Minor in Data Science\nResident of Clark I\n\nChadwick International School - Incheon, South Korea\n\nFull IB Diploma\nHiger Levels: Language & Literature, Global Politics, History\nStandard Levels: Biology, Math, Spanish\n\nDankook University Middle School - Seoul, South Korea\n\n2 years of extremely rigorous grind at the hub of Korean education\nOnly-boys middle school\nEndless hours of private academies (hagwons)\n\nDaedo Elementary School - Seoul, South Korea\n\n5 years of extremely rigorous grind at the hub of Korean education\nEndless hours of private academies (hagwons)\n\nTransit Middle School - Buffalo, New York, USA\n\nWonderful break from Korean education\nA taste of cheerful life in US\n\nCountry Parkway Elementary School - Buffalo, New York, USA\n\nFreedom from Korean education in a peaceful town in Buffalo"
  },
  {
    "objectID": "about.html#work-experience",
    "href": "about.html#work-experience",
    "title": "Taehwan (Jack) Kim | 김태환",
    "section": "Work Experience",
    "text": "Work Experience\nKorea Search Fund\n\nCrucial part of a live deal\nWorking currently in Korean hours\n\nTortoise Capital Management\n\nDeveloped a passion for value investing\nBirth of a stock guru\n\nKim & Chang Law Firm\n\nExperienced the challenges of M&A deals\nImportance of connections in completing business deals\n\nDoOne Accounting Corporation\n\nEntire global financial system is powered by Excel 2021\nWork, Drink, Work more culture in Korea"
  },
  {
    "objectID": "about.html#activities-achievements",
    "href": "about.html#activities-achievements",
    "title": "Taehwan (Jack) Kim | 김태환",
    "section": "Activities & Achievements",
    "text": "Activities & Achievements\nStudent Unior for Reasonable Investment (SURI)\n\nExecutive Investment Manager of $1 million portfolio\n\nCertified Investment Manager\n\nCertified by Korean Financial Investment Association (KOFIA)"
  },
  {
    "objectID": "about.html#soccer-career",
    "href": "about.html#soccer-career",
    "title": "Taehwan (Jack) Kim | 김태환",
    "section": "Soccer Career",
    "text": "Soccer Career\nDaedo Elementary (2009-2013)\nSinjeong Elementary (2012)\nDakook Middle (2014-2015)\nChadwick International Varsity Soccer (2016-2020)\nSiheung FC U16, U17 (2015-2018)\nSongdo FC (2015-2020)\nPomona-Pitzer Varsity Soccer (2020-2022)"
  },
  {
    "objectID": "about.html#military-experience",
    "href": "about.html#military-experience",
    "title": "Taehwan (Jack) Kim | 김태환",
    "section": "Military Experience",
    "text": "Military Experience\nRepubic of Korea Army: Army Special Forces Reconnaissance Unit of 39th Infantry Brigade of 15th Infantry Division of Army 2nd Corps\n\nClassified missions"
  },
  {
    "objectID": "Project_5.html#project-1-updates",
    "href": "Project_5.html#project-1-updates",
    "title": "Project 5: Overview of DS2 - FA2025",
    "section": "Project 1 Updates",
    "text": "Project 1 Updates"
  },
  {
    "objectID": "Project_5.html#project-2-updates",
    "href": "Project_5.html#project-2-updates",
    "title": "Project 5: Overview of DS2 - FA2025",
    "section": "Project 2 Updates",
    "text": "Project 2 Updates\nfff"
  },
  {
    "objectID": "Project_5.html#project-3-updates",
    "href": "Project_5.html#project-3-updates",
    "title": "Project 5: Overview of DS2 - FA2025",
    "section": "Project 3 Updates",
    "text": "Project 3 Updates"
  },
  {
    "objectID": "Project_5.html#project-4-updates",
    "href": "Project_5.html#project-4-updates",
    "title": "Project 5: Overview of DS2 - FA2025",
    "section": "Project 4 Updates",
    "text": "Project 4 Updates"
  },
  {
    "objectID": "case_study_2.html#referee-decisions-for-arsenal-in-2021-2022-season",
    "href": "case_study_2.html#referee-decisions-for-arsenal-in-2021-2022-season",
    "title": "Case Analysis 2: Premier League 2021-2022",
    "section": "",
    "text": "TidyTuesday Reference\nOriginal Data Source\nObjective: Which referees gave out the most yellow cards to Arsenal during the 2021-2022 season?\nRefereeing decisions play huge outcome in a football match. In fact, fans and analysts alike do pay enormous attention to the designated referee for each fixture. As a lifelong Arsenal fan, I wanted to find out which referees tend to be harsher on Arsenal.\n\n\n\n\n\n\n\n\n\n\n\nThe bar chart represents the total number of yellow cards each referee has given to Arsenal. Craig Pawson handed out the most yellow cards to Arsenal, with 9 cards given.\n\n\n\n\n\n\n\n\n\n\n\n\nThis study offers insight into if Arsenal experienced home advantage in referee decisions. Many fans assume that referees tend to be more lenient toward home teams. This may manifest the most in referee decisions for making disciplinary actions. The bar chart shows the number of yellow cards Arsenal received depending on match venue.\nInterestingly, Arsenal received 29 yellow cards at away matches and 31 in home matches. This goes against common logic. Although not conclusive, home advantage did not work in Arsenal’s favor."
  },
  {
    "objectID": "case_study_1.html#successful-countries-and-athletes-in-the-olympics",
    "href": "case_study_1.html#successful-countries-and-athletes-in-the-olympics",
    "title": "Case Analysis 1: Olympics",
    "section": "",
    "text": "TidyTuesday Reference\nOriginal Data Source\nObjective: Usually it is the Olympic athletes that receive the fame. However, at the end of the day they each represent their country with pride. Let’s find out which nations are represented the most in the top 10 most successful athlete list (measured by medal count). So, which countries have produced the best talents?\n\n\n\nTop 10 Athletes by Total Medals (USSR/Russia Combined)\n\n\nName\nTeam\nTotal Medals\n\n\n\n\nMichael Fred Phelps, II\nUnited States\n28\n\n\nLarysa Semenivna Latynina (Diriy-)\nUSSR/Russia\n18\n\n\nNikolay Yefimovich Andrianov\nUSSR/Russia\n15\n\n\nBorys Anfiyanovych Shakhlin\nUSSR/Russia\n13\n\n\nEdoardo Mangiarotti\nItaly\n13\n\n\nOle Einar Bjrndalen\nNorway\n13\n\n\nTakashi Ono\nJapan\n13\n\n\nAleksey Yuryevich Nemov\nUSSR/Russia\n12\n\n\nDara Grace Torres (-Hoffman, -Minas)\nUnited States\n12\n\n\nJennifer Elisabeth \"Jenny\" Thompson (-Cumpelik)\nUnited States\n12\n\n\n\n\n\n\n\n\nTop Nations by Number of Top Athletes (USSR/Russia Combined)\n\n\nTeam\nNumber of Athletes\n\n\n\n\nUSSR/Russia\n4\n\n\nUnited States\n3\n\n\nItaly\n1\n\n\nJapan\n1\n\n\nNorway\n1\n\n\n\n\n\n\n\nAs for most successful athletes, Michael Phelps by far has won the most medals (28). His success if unprecedented, with the second most decorated athlete having a whopping 10 fewer medals than Phelps.\n\n\n\n\n\n\n\n\n\nRussia (formerly USSR) produced the highest number of most successful athletes. Russia/USSR and the US usually rank top 2 for medal counts. It is actually surprising to find China not represented in the list, given the recent success of China in Olympics."
  },
  {
    "objectID": "case_study_3.html#some-trends-in-netflix-movies",
    "href": "case_study_3.html#some-trends-in-netflix-movies",
    "title": "Case Analysis 3: Netflix Movies",
    "section": "",
    "text": "TidyTuesday Reference\nOriginal Data Source\nObjective: Analyze some interesting trends in Netflix movies as of 2019\nThis dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search engine.\nIn 2018, Flixable released an interesting report which shows that the number of TV shows on Netflix has nearly tripled since 2010. The streaming service’s number of movies has decreased by more than 2,000 titles since 2010, while its number of TV shows has nearly tripled. It will be interesting to explore what other insights can be obtained from the same dataset.\nIntegrating this dataset with other external datasets such as IMDB ratings, rotten tomatoes can also provide many interesting findings.\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-04-20')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 17)\n\nnetflix &lt;- tuesdata$netflix\n\nlibrary(tidyverse)\n\n\n\n\nactor_count &lt;- netflix |&gt;\n  filter(type == \"Movie\", !is.na(cast)) |&gt; \n  separate_rows(cast, sep = \",\\\\s*\") |&gt; \n  count(cast, sort = TRUE)\n\ntop_5_actors &lt;- actor_count |&gt;\n  slice_head(n = 5) \n\n\nggplot(top_5_actors, aes(x = reorder(cast, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +  \n  labs(\n    title = \"Top 5 Actors Appearing in Netflix Titles\",\n    x = \"Actor\",\n    y = \"Number of Appearances\"\n  ) +\n  theme_minimal() \n\n\n\n\n\n\n\n\nInteresting, I actually do not know any of these actors/actresses. Upon some research, I have found that all of them are Indian actors/actresses. These actors/actresses were very active in their respective careers.\nAs a side analysis, let’s find out which countries produced the most number of movies?\n\ncountry_count_movies &lt;- netflix |&gt;\n  filter(type == \"Movie\", !is.na(country)) |&gt;  \n  separate_rows(country, sep = \",\\\\s*\") |&gt;  \n  count(country, sort = TRUE)  \n\ntop_5_countries &lt;- country_count_movies |&gt;\n  slice_head(n = 5)  \n\n\nggplot(top_5_countries, aes(x = reorder(country, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +  \n  labs(\n    title = \"Top 5 Countries Producing the Most Netflix Movies\",\n    x = \"Country\",\n    y = \"Number of Movies\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nUnsurprisingly, the US has produced the most Netflix movies. This makes sense given that Hollywood is the global hub of film production. The US has produced more than double the amount of movies than second place India and more than the movie productions of the next four countries combined. With the growing size of Bollywood, India comes in for second place. This may speak to the high number of Indian actors/actresses in the first study.\n\n\n\nWe have excluded articles, pronouns, prepositions and any common stop words.\n\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(stringr)\n\n# A list of words we want to exclude\nstop_words &lt;- c(\"a\", \"an\", \"the\", \"and\", \"in\",\n                \"on\", \"of\", \"with\", \"for\", \"to\",\n                \"he\", \"she\", \"my\", \"it\", \"him\",\n                \"her\", \"above\", \"below\", \"top\",\n                \"bottom\", \"between\", \"front\", \"back\",\n                \"beneath\", \"they\", \"me\", \"you\", \"them\",\n                \"us\", \"we\", \"that\", \"this\", \"these\",\n                \"those\", \"I\", \"from\", \"i\", \"at\", \"\")\n\n# Step 1: Filter and preprocess the titles\nword_count &lt;- netflix |&gt;\n  subset(type == \"Movie\" & !is.na(title)) |&gt;\n  transform(clean_title = str_remove_all(title, \"[^A-Za-z\\\\s]\")) |&gt;\n  transform(clean_title = tolower(clean_title)) |&gt;\n  {\\(df) data.frame(clean_title = unlist(strsplit(df$clean_title, \"\\\\s+\")))}() |&gt;\n  subset(!clean_title %in% stop_words) |&gt;\n  table() |&gt;\n  as.data.frame()\n\n# Step 2: Sort and rename columns\nword_count &lt;- word_count[order(-word_count$Freq), ]\ncolnames(word_count) &lt;- c(\"Word\", \"Frequency\")\n\n# Step 3: Display top 10 words as a formatted table\nword_count |&gt;\n  head(10) |&gt;\n  kable(\n    caption = \"&lt;strong style='color:black; font-size:16px;'&gt;Top 10 Most Frequent Words in Netflix Movie Titles&lt;/strong&gt;\",\n    col.names = c(\"Word\", \"Frequency\"),\n    row.names = FALSE,  # Suppress row numbers\n    align = c(\"l\", \"c\"),\n    escape = FALSE\n  ) |&gt;\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),\n    full_width = FALSE,\n    position = \"center\"\n  )\n\n\nTop 10 Most Frequent Words in Netflix Movie Titles\n\n\nWord\nFrequency\n\n\n\n\nlove\n91\n\n\nchristmas\n72\n\n\nmovie\n60\n\n\nman\n58\n\n\nstory\n54\n\n\nlife\n44\n\n\nworld\n42\n\n\nlittle\n39\n\n\nlive\n38\n\n\none\n38\n\n\n\n\n\n\n\nLove and Christmas are the top two most common words. These results do makes sense as these are popular topics.\n\n\n\nThe most commercially successful filmmaker in the history of Hollywood is Steven Spielberg. Steven Spielberg is the creator of many of the most successful franchises and stand-alone films. They include the Indiana Jones series, the Jurassic Park series, Saving Private Ryan, and Jaws. Let’s figure out which of his movies are on Netflix.\n\nspielberg_movies &lt;- netflix |&gt;\n  filter(type == \"Movie\", str_detect(director, \"Steven Spielberg\"))\n\nspielberg_movies |&gt;\n  select(title) |&gt;  # Select only the title column\n  rename(\"Title\" = title) |&gt;  # Rename the column for clarity\n  kable(\n    caption = \"&lt;strong style='color:black; font-size:16px;'&gt;Steven Spielberg Movies Available on Netflix&lt;/strong&gt;\",\n    col.names = c(\"Title\"),\n    align = \"l\",\n    escape = FALSE \n  ) |&gt;\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),\n    full_width = FALSE,\n    position = \"center\"\n  )\n\n\nSteven Spielberg Movies Available on Netflix\n\n\nTitle\n\n\n\n\nCatch Me If You Can\n\n\nHook\n\n\nIndiana Jones and the Kingdom of the Crystal Skull\n\n\nIndiana Jones and the Last Crusade\n\n\nIndiana Jones and the Raiders of the Lost Ark\n\n\nIndiana Jones and the Temple of Doom\n\n\nLincoln\n\n\nSchindler's List\n\n\nThe Adventures of Tintin\n\n\nWar Horse\n\n\n\n\n\n\n\nSurprising, only 10 are available on Netflix. Given sheer amount of Spielberg movies we know of, this a very small number."
  },
  {
    "objectID": "case_study_3.html#too-lazy-for-theaters.-when-can-i-watch-a-new-movie-on-netflix",
    "href": "case_study_3.html#too-lazy-for-theaters.-when-can-i-watch-a-new-movie-on-netflix",
    "title": "Case Analysis 3: Netflix Movies",
    "section": "4. Too lazy for theaters. When can I watch a new movie on Netflix?",
    "text": "4. Too lazy for theaters. When can I watch a new movie on Netflix?\nFollowing Covid-19, fewer people are going to the movies. At the same time over-the-top streaming services have become more influential. Nowadays, people are wondering “when is my favorite movie going to be on Netflix?”. So let’s find out, on average, how long does it take for a movie to be added on Netflix.\nTwo caveats: - We have exact dates for the when the movie was added, but only the year data is available for when it was released. Hence, we will just find the number of years. - Netflix started streaming services in 2007. Any movie produced before 2007 will be excluded because that just automatically increases the average.\n\nlibrary(lubridate)\nlibrary(knitr)\nlibrary(kableExtra)\n\nnetflix_movies &lt;- netflix |&gt;\n  subset(type == \"Movie\" & !is.na(date_added) & !is.na(release_year)) |&gt;\n  transform(\n    date_added = mdy(date_added),\n    release_year = as.numeric(release_year)\n  )\n\nnetflix_movies &lt;- netflix_movies |&gt;\n  transform(\n    added_year = year(date_added)\n  )\n\nnetflix_movies &lt;- netflix_movies |&gt;\n  transform(\n    time_diff_years = added_year - release_year\n  )\n\nnetflix_movies_filtered &lt;- netflix_movies |&gt;\n  subset(!is.na(time_diff_years) & time_diff_years &gt;= 0 & release_year &gt;= 2007)\n\naverage_time_diff &lt;- mean(netflix_movies_filtered$time_diff_years, na.rm = TRUE)\n\ndata.frame(`Average Time Difference (Years)` = round(average_time_diff, 2)) |&gt;\n  kable(\n    col.names = c(\"Average Time Difference (Years)\"),\n    align = \"c\"\n  ) |&gt;\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),\n    full_width = FALSE,\n    position = \"center\"\n  )\n\n\n\n\nAverage Time Difference (Years)\n\n\n\n\n2.46\n\n\n\n\n\n\n\nOn average, it takes 2.46 years for a movie to be added to Netflix after being released in theaters.\nAdditionally, let’s look at how the average time differ across content ratings. The motion picture content ratings used to indicate the suitability of a movie for different audiences. Below are the description of each rating retrieved from Spectrum.\n\nG: General audiences, suitable for all ages\nPG: Parental guidance suggested, some material may not be suitable for children\nPG-13: Parents strongly cautioned, some material may be inappropriate for children under 13\nR: Restricted, under 17 requires an accompanying parent or adult guardian\nNC-17: Clearly adult\nUR: Unrated\nNR: Not rated (not officially assigned)\nTV-Y: All children\nTV-Y7: Directed to older children\nTV-Y7-FV: Directed to older children - fantasy violence\nTV-G: General audience\nTV-PG: Parental guidance suggested\nTV-14: Parents strongly cautioned\nTV-MA: Mature audience only\n\n\nlibrary(lubridate)\nlibrary(ggplot2)\n\n\nmovies_data &lt;- netflix |&gt;\n  subset(type == \"Movie\" & !is.na(date_added) & !is.na(release_year)) |&gt;\n  transform(\n    date_added = mdy(date_added),\n    release_year = as.numeric(release_year)\n  )\n\n\nmovies_data &lt;- movies_data |&gt;\n  transform(\n    added_year = year(date_added)\n  )\n\n\nmovies_data &lt;- movies_data |&gt;\n  transform(\n    time_diff_years = added_year - release_year\n  )\n\n\nmovies_filtered &lt;- movies_data |&gt;\n  subset(!is.na(time_diff_years) & time_diff_years &gt;= 0 & release_year &gt;= 2007)\n\n\naverage_time_by_rating_table &lt;- aggregate(\n  time_diff_years ~ rating,\n  data = movies_filtered,\n  FUN = function(x) mean(x, na.rm = TRUE)\n)\n\n\naverage_time_by_rating_table &lt;- average_time_by_rating_table[order(-average_time_by_rating_table$time_diff_years), ]\n\n\nggplot(average_time_by_rating_table, aes(x = reorder(rating, -time_diff_years), y = time_diff_years, fill = rating)) +\n  geom_bar(stat = \"identity\", width = 0.7, show.legend = FALSE) +\n  geom_text(aes(label = round(time_diff_years, 2)), vjust = -0.5, size = 5) +\n  labs(\n    title = \"Average Time Difference by Rating (Movies Only)\",\n    x = \"Rating\",\n    y = \"Average Time Difference (Years)\"\n  ) +\n  theme_minimal() +\n  theme(\n    text = element_text(size = 14),\n    plot.title = element_text(hjust = 0.5),\n    axis.text.x = element_text(angle = 45, hjust = 1, size = 12),\n    plot.margin = margin(10, 10, 30, 10)\n  ) +\n  coord_cartesian(clip = \"off\")\n\n\n\n\n\n\n\n\nUnrated movies (UR) exhibited the longest wait time for the movie to be uploaded on Netflix. This was followed by PG-13 rated movies and PG rated movies. UR, PG-13, PG, R, G, TV-14, and NR rated movies all took longer than the average time for all movies."
  },
  {
    "objectID": "Project_3.html#are-departure-delays-significantly-different-between-morning-and-evening-flights",
    "href": "Project_3.html#are-departure-delays-significantly-different-between-morning-and-evening-flights",
    "title": "Project 3: NYC Flights Analysis",
    "section": "",
    "text": "Objective: In this analysis, I will investigate whether there is a significant difference in departure delays between morning and evening flights using data from the nycflights13 package. To do this, I will perform a permutation test, where I will shuffle the time-of-day labels (morning/evening) and repeatedly calculate the difference in mean departure delays. This will help simulate the null hypothesis, which assumes that the time of day has no statistical significance on departure delays. By comparing the observed difference in delays to this simulated distribution, I will assess the statistical significance of the observed difference.\n[Null Hypothesis] There is no difference in the mean departure delays between morning flights and evening flights.\n\nlibrary(nycflights13)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(DT)\n\n\nflights &lt;- nycflights13::flights |&gt;\n  filter(!is.na(dep_delay)) |&gt;\n  mutate(time_of_day = case_when(\n    hour &gt;= 5 & hour &lt; 11 ~ \"morning\",\n    hour &gt;= 17 & hour &lt; 23 ~ \"evening\",\n    TRUE ~ \"other\"\n  )) |&gt;\n  filter(time_of_day != \"other\")  # Exclude flights that are not in the morning or evening\n\n\n# Display the first 5 rows as an interactive table\ndatatable(\n  flights[1:5, ],  # Select the first 5 rows\n  options = list(\n    pageLength = 5,      # Display 5 rows per page\n    dom = 't',           # Show only the table (no search box, etc.)\n    scrollX = TRUE       # Enable horizontal scrolling\n  ),\n  caption = \"First 5 Rows of Flights Data\"\n)\n\n\n\n\n\nWe are using the nycflights13 data which contains the flight information in New York airports in 2013. Most importantly, the data shows the departure delay time (in minutes). We will first define morning and evening flights. Morning flights are flights departing between 5:00am and 11:00am. Evening flights are flights departing from 5:00pm and 11:00pm (ex. First row, dep_time 517 means departure time is at 5:17am). We removed the flights that are not within these time ranges.\nBelow we have the average delayed time for morning and evening flights.\n\nlibrary(knitr)\nlibrary(kableExtra)\n\nmean_delays &lt;- flights |&gt;\n  group_by(time_of_day) |&gt;\n  summarize(mean_delay = mean(dep_delay, na.rm = TRUE))\n\nmean_delays |&gt;\n  kable(\n    caption = \"Mean Departure Delay by Time of Day\",\n    col.names = c(\"Time of Day\", \"Mean Delay (minutes)\"),\n    align = c(\"l\", \"c\")\n  ) |&gt;\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),\n    full_width = FALSE,\n    position = \"center\"\n  )\n\n\nMean Departure Delay by Time of Day\n\n\nTime of Day\nMean Delay (minutes)\n\n\n\n\nevening\n22.729753\n\n\nmorning\n3.492098\n\n\n\n\n\n\n\nThen we calculate the observed differences.\n\nobs_diff &lt;- flights |&gt;\n  group_by(time_of_day) |&gt;\n  summarize(mean_delay = mean(dep_delay)) |&gt;\n  summarize(diff = diff(mean_delay)) |&gt;\n  pull(diff)\n\n\n\nObserved Difference in Mean Delays (Evening - Morning): -19.23765 minutes\n\n\n[Permutation Test] We’ll shuffle the time labels (morning or evening) 1000 times and calculate the difference in mean delays for each permutation.\n\nset.seed(47)\n\nnull_dist &lt;- map_dbl(1:1000, ~ {\n  flights |&gt; \n    mutate(shuffled_time = sample(time_of_day)) |&gt; \n    group_by(shuffled_time) |&gt; \n    summarize(mean_delay = mean(dep_delay), .groups = \"drop\") |&gt; \n    summarize(diff = diff(mean_delay)) |&gt; \n    pull(diff)\n})\n\nLet’s plot the null distribution with observed difference\n\nggplot(data.frame(null_dist), aes(x = null_dist)) +\n  geom_histogram(binwidth = 0.5, fill = \"lightblue\", color = \"black\") +\n  geom_vline(xintercept = obs_diff, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Permutation Test: Morning vs Evening Delays\",\n    x = \"Difference in Mean Delays\",\n    y = \"Frequency\"\n  )\n\n\n\n\n\n\n\n\n\n# Calculate p-value\np_value &lt;- mean(abs(null_dist) &gt;= abs(obs_diff))\n\n\n\nP-value: 0 \n\n\nResults\nThe red dashed line represents the observed difference in mean delays between morning and evening flights. The histogram represents the differences in mean delays generated under the null hypothesis (shuffling the time labels).\nA p-value of 0 means that none of the 1000 permutations generated a difference in mean delays as extreme (or more extreme) as the observed difference. This indicates that the observed difference is highly unlikely to have occurred by chance if the null hypothesis were true.\nSince the p-value is 0, this provides strong evidence against the null hypothesis. You would reject the null hypothesis and conclude that the time of day (morning vs. evening) may be correlated to departure delays.\nThe visualization supports this as the red line falls outside the range of the simulated null distribution values, highlighting that the observed difference is not consistent with the null hypothesis assumption.\nGreater Insights\nSo how can we interpret these results in a broader context? The current analysis is based on flights departing from NYC airports (JFK, LGA, EWR) in 2013 and data acquired under the assumption that morning and evening flights do not have missing delay information. This means the conclusion of a significant difference in mean delays applies only to NYC flights in 2013.\nFor potential larger populations, we could generalize the results to other years in NYC. Similar patterns of delays (morning flights being less delayed than evening flights) might hold for other years if no major changes occurred in scheduling, infrastructure, or air traffic. It may also be possible to generalize to similar metropolitan hubs. NYC is one of the busiest air travel hubs in the US, so patterns observed in NYC might apply to other large cities with comparable air traffic and flight schedules, such as Chicago (ORD, MDW), Los Angeles (LAX), or Atlanta (ATL). However, generalization should be done cautiously and supported by additional evidence. Moreover, there may be limitations to generalization in the sense that regional factors may come into play. NYC has unique air traffic patterns due to its size, location, and volume of international and domestic flights. Other cities may not share these characteristics."
  }
]