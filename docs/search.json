[
  {
    "objectID": "case_study_1.html",
    "href": "case_study_1.html",
    "title": "Case Analysis 1: Olympics",
    "section": "",
    "text": "TidyTuesday Reference\nOriginal Data Source\nObjective: Usually it is the Olympic athletes that receive the fame. However, at the end of the day they each represent their country with pride. Let’s find out which nations are represented the most in the top 10 most successful athlete list (measured by medal count). So, which countries have produced the best talents?\n\n\n# A tibble: 10 × 3\n   name                                                team         total_medals\n   &lt;chr&gt;                                               &lt;chr&gt;               &lt;int&gt;\n 1 \"Michael Fred Phelps, II\"                           United Stat…           28\n 2 \"Larysa Semenivna Latynina (Diriy-)\"                Soviet Union           18\n 3 \"Nikolay Yefimovich Andrianov\"                      Soviet Union           15\n 4 \"Borys Anfiyanovych Shakhlin\"                       Soviet Union           13\n 5 \"Edoardo Mangiarotti\"                               Italy                  13\n 6 \"Ole Einar Bjrndalen\"                               Norway                 13\n 7 \"Takashi Ono\"                                       Japan                  13\n 8 \"Aleksey Yuryevich Nemov\"                           Russia                 12\n 9 \"Dara Grace Torres (-Hoffman, -Minas)\"              United Stat…           12\n10 \"Jennifer Elisabeth \\\"Jenny\\\" Thompson (-Cumpelik)\" United Stat…           12\n\n\n# A tibble: 6 × 2\n  team          count\n  &lt;chr&gt;         &lt;int&gt;\n1 Soviet Union      3\n2 United States     3\n3 Italy             1\n4 Japan             1\n5 Norway            1\n6 Russia            1\n\n\n\n\n\n\n\n\n\n\n\nUSSR and United States produced the highest number of most successful athletes. Russia (formerly USSR) and the US usually rank top 3 for medal counts. It is actually surprising to find China not represented in the list.\nAs for most successful athletes, Michael Phelps is by far has won the most medals (28)."
  },
  {
    "objectID": "case_study_3.html",
    "href": "case_study_3.html",
    "title": "Case Analysis 3: Netflix Movies",
    "section": "",
    "text": "TidyTuesday Reference\nOriginal Data Source\nThis dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search engine.\nIn 2018, they released an interesting report which shows that the number of TV shows on Netflix has nearly tripled since 2010. The streaming service’s number of movies has decreased by more than 2,000 titles since 2010, while its number of TV shows has nearly tripled. It will be interesting to explore what all other insights can be obtained from the same dataset.\nIntegrating this dataset with other external datasets such as IMDB ratings, rotten tomatoes can also provide many interesting findings.\n\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-04-20')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 17)\n\nnetflix &lt;- tuesdata$netflix\n\nlibrary(tidyverse)\n\n\nWho are the actors with the most movie appearances? Let’s find the top 5 actors by the number of appearances.\n\n\nactor_count &lt;- netflix |&gt;\n  filter(type == \"Movie\", !is.na(cast)) |&gt; \n  separate_rows(cast, sep = \",\\\\s*\") |&gt; \n  count(cast, sort = TRUE)\n\ntop_5_actors &lt;- actor_count |&gt;\n  slice_head(n = 5) \n\n\nggplot(top_5_actors, aes(x = reorder(cast, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +  \n  labs(\n    title = \"Top 5 Actors Appearing in Netflix Titles\",\n    x = \"Actor\",\n    y = \"Number of Appearances\"\n  ) +\n  theme_minimal() \n\n\n\n\n\n\n\n\nInteresting, I actually do not know any of these actors/actresses. Upon some research, I have found that all of them are Indian actors/actresses.\nJust out of curiosity, let’s find out which countries produced the most number of movies?\n\ncountry_count_movies &lt;- netflix |&gt;\n  filter(type == \"Movie\", !is.na(country)) |&gt;  \n  separate_rows(country, sep = \",\\\\s*\") |&gt;  \n  count(country, sort = TRUE)  \n\ntop_5_countries &lt;- country_count_movies |&gt;\n  slice_head(n = 5)  \n\n\nggplot(top_5_countries, aes(x = reorder(country, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +  \n  labs(\n    title = \"Top 5 Countries Producing the Most Netflix Movies\",\n    x = \"Country\",\n    y = \"Number of Movies\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nUnsurprisingly, the US has produced the most Netflix movies.This makes sense given that Hollywood is the global hub of film production. With the growing size of Bollywood, India comes in for second place.\n\nMost popular words in movie titles:\n\nWe have excluded articles, pronouns, prepositions and any common stop words.\n\n# A list of words we want to exclude\nstop_words &lt;- c(\"a\", \"an\", \"the\", \"and\", \"in\", \"on\", \"of\", \"with\", \"for\", \"to\", \"he\", \"she\", \"my\", \"it\", \"him\", \"her\", \"above\", \"below\", \"top\", \"bottom\", \"between\", \"front\", \"back\", \"beneath\", \"they\", \"me\", \"you\", \"them\", \"us\", \"we\", \"that\", \"this\", \"these\", \"those\", \"I\", \"from\", \"i\", \"at\", \"\")\n\nword_count &lt;- netflix |&gt;\n  filter(type == \"Movie\", !is.na(title)) |&gt;\n  mutate(clean_title = str_remove_all(title, \"[^A-Za-z\\\\s]\")) |&gt;\n  mutate(clean_title = str_to_lower(clean_title)) |&gt;\n  separate_rows(clean_title, sep = \"\\\\s+\") |&gt; \n  filter(!clean_title %in% stop_words) |&gt; \n  count(clean_title, sort = TRUE) \n\nhead(word_count, 10)\n\n# A tibble: 10 × 2\n   clean_title     n\n   &lt;chr&gt;       &lt;int&gt;\n 1 love           91\n 2 christmas      72\n 3 movie          60\n 4 man            58\n 5 story          54\n 6 life           44\n 7 world          42\n 8 little         39\n 9 live           38\n10 one            38\n\n\nLove and Christmas are the top two most common words. These results do makes sense as these are pretty popular topics.\n\nThe most commercially successful filmmaker in the history of Hollywood is Steven Spielburg. Let’s figure out which of his movies are on Netflix.\n\n\nspielberg_movies &lt;- netflix |&gt;\n  filter(type == \"Movie\", str_detect(director, \"Steven Spielberg\"))\n\nhead(spielberg_movies$title)\n\n[1] \"Catch Me If You Can\"                               \n[2] \"Hook\"                                              \n[3] \"Indiana Jones and the Kingdom of the Crystal Skull\"\n[4] \"Indiana Jones and the Last Crusade\"                \n[5] \"Indiana Jones and the Raiders of the Lost Ark\"     \n[6] \"Indiana Jones and the Temple of Doom\"              \n\n\nSurprising, only 6 are available on Netflix. Given sheer amount of Spielberg movies we know of, this a very small number.\n\nFollowing Covid-19, less people are going to the movies. At the same time over-the-top streaming services have become more influential. Nowadays, people are wondering “when is my favorite movie going to be on Netflix?”. So let’s find out, on average, how long does it take for a movie to be added on Netflix.\n\nTwo caveats: - We have exact dates for the when the movie was added, but only the year data is available for when it was released. Hence, we will just find the number of years. - Netflix started streaming services in 2007. Any movie produced before 2007 will be excluded because that just automatically increases the average.\n\nnetflix &lt;- netflix |&gt;\n  mutate(\n    date_added = mdy(date_added),  \n    release_year = as.numeric(release_year)  \n  )\n\nnetflix &lt;- netflix |&gt;\n  mutate(added_year = year(date_added))  \n\nnetflix &lt;- netflix |&gt;\n  mutate(time_diff_years = added_year - release_year)  \n\nnetflix_era &lt;- netflix |&gt;\n  filter(!is.na(time_diff_years) & time_diff_years &gt;= 0 & release_year &gt;= 2007) \n\naverage_time_diff &lt;- netflix_era |&gt;\n  summarize(average_diff = mean(time_diff_years, na.rm = TRUE))\n\naverage_time_diff\n\n# A tibble: 1 × 1\n  average_diff\n         &lt;dbl&gt;\n1         2.14\n\n\nOn average, it takes 2.14 years for a movie to be added on Netflix after being released in the theaters."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Taehwan (Jack) Kim",
    "section": "",
    "text": "Greetings! I’m Jack. Junior and Economics major at Pomona College. Formerly of Class of 2024, finally back at school after serving in the Republic of Korea Army. My interest lies in the world of finance and value investing. Love to play soccer (lifelong Arsenal fan) and love to share my military endeavors. This is a space to test my analytical skillset."
  },
  {
    "objectID": "Project_2.html",
    "href": "Project_2.html",
    "title": "Project_2",
    "section": "",
    "text": "TidyTuesday Reference\nOriginal Data Source\nThis dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search engine.\nIn 2018, they released an interesting report which shows that the number of TV shows on Netflix has nearly tripled since 2010. The streaming service’s number of movies has decreased by more than 2,000 titles since 2010, while its number of TV shows has nearly tripled. It will be interesting to explore what all other insights can be obtained from the same dataset.\nIntegrating this dataset with other external datasets such as IMDB ratings, rotten tomatoes can also provide many interesting findings.\n\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-04-20')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 17)\n\nnetflix &lt;- tuesdata$netflix\n\nlibrary(tidyverse)\n\n\nWho are the actors with the most movie appearances? Let’s find the top 5 actors by the number of appearances.\n\n\nactor_count &lt;- netflix |&gt;\n  filter(type == \"Movie\", !is.na(cast)) |&gt; \n  separate_rows(cast, sep = \",\\\\s*\") |&gt; \n  count(cast, sort = TRUE)\n\ntop_5_actors &lt;- actor_count |&gt;\n  slice_head(n = 5) \n\n\nggplot(top_5_actors, aes(x = reorder(cast, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +  \n  labs(\n    title = \"Top 5 Actors Appearing in Netflix Titles\",\n    x = \"Actor\",\n    y = \"Number of Appearances\"\n  ) +\n  theme_minimal() \n\n\n\n\n\n\n\n\nInteresting, I actually do not know any of these actors/actresses. Upon some research, I have found that all of them are Indian actors/actresses.\nJust out of curiosity, let’s find out which countries produced the most number of movies?\n\ncountry_count_movies &lt;- netflix |&gt;\n  filter(type == \"Movie\", !is.na(country)) |&gt;  \n  separate_rows(country, sep = \",\\\\s*\") |&gt;  \n  count(country, sort = TRUE)  \n\ntop_5_countries &lt;- country_count_movies |&gt;\n  slice_head(n = 5)  \n\n\nggplot(top_5_countries, aes(x = reorder(country, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +  \n  labs(\n    title = \"Top 5 Countries Producing the Most Netflix Movies\",\n    x = \"Country\",\n    y = \"Number of Movies\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nUnsurprisingly, the US has produced the most Netflix movies.This makes sense given that Hollywood is the global hub of film production. With the growing size of Bollywood, India comes in for second place.\n\nMost popular words in movie titles:\n\nWe have excluded articles, pronouns, prepositions and any common stop words.\n\n# A list of words we want to exclude\nstop_words &lt;- c(\"a\", \"an\", \"the\", \"and\", \"in\", \"on\", \"of\", \"with\", \"for\", \"to\", \"he\", \"she\", \"my\", \"it\", \"him\", \"her\", \"above\", \"below\", \"top\", \"bottom\", \"between\", \"front\", \"back\", \"beneath\", \"they\", \"me\", \"you\", \"them\", \"us\", \"we\", \"that\", \"this\", \"these\", \"those\", \"I\", \"from\", \"i\", \"at\", \"\")\n\nword_count &lt;- netflix |&gt;\n  filter(type == \"Movie\", !is.na(title)) |&gt;\n  mutate(clean_title = str_remove_all(title, \"[^A-Za-z\\\\s]\")) |&gt;\n  mutate(clean_title = str_to_lower(clean_title)) |&gt;\n  separate_rows(clean_title, sep = \"\\\\s+\") |&gt; \n  filter(!clean_title %in% stop_words) |&gt; \n  count(clean_title, sort = TRUE) \n\nhead(word_count, 10)\n\n# A tibble: 10 × 2\n   clean_title     n\n   &lt;chr&gt;       &lt;int&gt;\n 1 love           91\n 2 christmas      72\n 3 movie          60\n 4 man            58\n 5 story          54\n 6 life           44\n 7 world          42\n 8 little         39\n 9 live           38\n10 one            38\n\n\nLove and Christmas are the top two most common words. These results do makes sense as these are pretty popular topics.\n\nThe most commercially successful filmmaker in the history of Hollywood is Steven Spielburg. Let’s figure out which of his movies are on Netflix.\n\n\nspielberg_movies &lt;- netflix |&gt;\n  filter(type == \"Movie\", str_detect(director, \"Steven Spielberg\"))\n\nhead(spielberg_movies$title)\n\n[1] \"Catch Me If You Can\"                               \n[2] \"Hook\"                                              \n[3] \"Indiana Jones and the Kingdom of the Crystal Skull\"\n[4] \"Indiana Jones and the Last Crusade\"                \n[5] \"Indiana Jones and the Raiders of the Lost Ark\"     \n[6] \"Indiana Jones and the Temple of Doom\"              \n\n\nSurprising, only 6 are available on Netflix. Given sheer amount of Spielberg movies we know of, this a very small number.\n\nFollowing Covid-19, less people are going to the movies. At the same time over-the-top streaming services have become more influential. Nowadays, people are wondering “when is my favorite movie going to be on Netflix?”. So let’s find out, on average, how long does it take for a movie to be added on Netflix.\n\nTwo caveats: - We have exact dates for the when the movie was added, but only the year data is available for when it was released. Hence, we will just find the number of years. - Netflix started streaming services in 2007. Any movie produced before 2007 will be excluded because that just automatically increases the average.\n\nnetflix &lt;- netflix |&gt;\n  mutate(\n    date_added = mdy(date_added),  \n    release_year = as.numeric(release_year)  \n  )\n\nnetflix &lt;- netflix |&gt;\n  mutate(added_year = year(date_added))  \n\nnetflix &lt;- netflix |&gt;\n  mutate(time_diff_years = added_year - release_year)  \n\nnetflix_era &lt;- netflix |&gt;\n  filter(!is.na(time_diff_years) & time_diff_years &gt;= 0 & release_year &gt;= 2007) \n\naverage_time_diff &lt;- netflix_era |&gt;\n  summarize(average_diff = mean(time_diff_years, na.rm = TRUE))\n\naverage_time_diff\n\n# A tibble: 1 × 1\n  average_diff\n         &lt;dbl&gt;\n1         2.14\n\n\nOn average, it takes 2.14 years for a movie to be added on Netflix after being released in the theaters."
  },
  {
    "objectID": "Project_3.html",
    "href": "Project_3.html",
    "title": "Project 3: NYC Flights Analysis",
    "section": "",
    "text": "Are departure delays significantly different between morning and evening flights?\n[Objective] In this analysis, I will investigate whether there is a significant difference in departure delays between morning and evening flights using data from the nycflights13 package. To do this, I will perform a permutation test, where I will shuffle the time-of-day labels (morning/evening) and repeatedly calculate the difference in mean departure delays. This will help simulate the null hypothesis, which assumes that the time of day has no effect on departure delays. By comparing the observed difference in delays to this simulated distribution, I will assess the statistical significance of the observed difference.\n[Null Hypothesis] Time of day (morning vs. evening) has no effect on departure delays.\n\nlibrary(nycflights13)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(ggplot2)\n\n\nflights &lt;- nycflights13::flights |&gt;\n  filter(!is.na(dep_delay)) |&gt;\n  mutate(time_of_day = case_when(\n    hour &gt;= 5 & hour &lt; 11 ~ \"morning\",\n    hour &gt;= 17 & hour &lt; 23 ~ \"evening\",\n    TRUE ~ \"other\"\n  )) |&gt;\n  filter(time_of_day != \"other\")  # Exclude flights that are not in the morning or evening\n\nhead(flights, 10)\n\n# A tibble: 10 × 20\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 12 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, time_of_day &lt;chr&gt;\n\n\nWe are using the nycflights13 data which contains the flight information in New York airports in 2013. Most importantly, the data shows the departure delay time (in minutes). We will first define morning and evening flights. Morning flights are flights departing between 5:00am and 11:00am. Evening flights are flights departing from 5:00pm and 11:00pm (ex. First row, dep_time 517 means departure time is at 5:17am). We removed the flights that are not within these time ranges.\nBelow we have the average delayed time for morning and evening flights.\n\nmean_delays &lt;- flights |&gt;\n  group_by(time_of_day) |&gt;\n  summarize(mean_delay = mean(dep_delay, na.rm = TRUE))\n\nprint(mean_delays)\n\n# A tibble: 2 × 2\n  time_of_day mean_delay\n  &lt;chr&gt;            &lt;dbl&gt;\n1 evening          22.7 \n2 morning           3.49\n\n\nThen we calculate the observed differences.\n\nobs_diff &lt;- flights |&gt;\n  group_by(time_of_day) |&gt;\n  summarize(mean_delay = mean(dep_delay)) |&gt;\n  summarize(diff = diff(mean_delay)) |&gt;\n  pull(diff)\n\nobs_diff\n\n[1] -19.23765\n\ncat(\"Observed Difference in Mean Delays (Evening - Morning):\", obs_diff, \"minutes\\n\")\n\nObserved Difference in Mean Delays (Evening - Morning): -19.23765 minutes\n\n\n[Permutation Test] We’ll shuffle the time labels (morning or evening) 1000 times and calculate the difference in mean delays for each permutation.\n\nset.seed(47)\n\nnull_dist &lt;- map_dbl(1:1000, ~ {\n  flights |&gt; \n    mutate(shuffled_time = sample(time_of_day)) |&gt; \n    group_by(shuffled_time) |&gt; \n    summarize(mean_delay = mean(dep_delay), .groups = \"drop\") |&gt; \n    summarize(diff = diff(mean_delay)) |&gt; \n    pull(diff)\n})\n\nhead(null_dist, 10)\n\n [1] -0.10935078 -0.22156561 -0.06383260  0.17758638  0.27808711 -0.24016464\n [7] -0.03553646 -0.04862754 -0.02979578  0.02311159\n\n\n\n# Plot the null distribution with observed difference\nggplot(data.frame(null_dist), aes(x = null_dist)) +\n  geom_histogram(binwidth = 0.5, fill = \"lightblue\", color = \"black\") +\n  geom_vline(xintercept = obs_diff, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Permutation Test: Morning vs Evening Delays\",\n    x = \"Difference in Mean Delays\",\n    y = \"Frequency\"\n  )\n\n\n\n\n\n\n\n\n\n# Calculate p-value\np_value &lt;- mean(abs(null_dist) &gt;= abs(obs_diff))\ncat(\"P-value:\", p_value, \"\\n\")\n\nP-value: 0 \n\n\nConclusion\nThe red dashed line represents the observed difference in mean delays between morning and evening flights. The histogram represents the differences in mean delays generated under the null hypothesis (shuffling the time labels).\nA p-value of 0 means that none of the 1000 permutations generated a difference in mean delays as extreme (or more extreme) as the observed difference. This indicates that the observed difference is highly unlikely to have occurred by chance if the null hypothesis were true.\nSince the p-value is 0, this provides strong evidence against the null hypothesis. You would reject the null hypothesis and conclude that the time of day (morning vs. evening) does have a significant effect on departure delays.\nThe visualization supports this as the red line falls outside the range of the simulated null distribution values, highlighting that the observed difference is not consistent with the null hypothesis assumption."
  },
  {
    "objectID": "case_study_2.html",
    "href": "case_study_2.html",
    "title": "Case Analysis 2: Premier League 2021-2022",
    "section": "",
    "text": "TidyTuesday Reference\nOriginal Data Source\nObjective of the Analysis: Which referee gave out the most yellow cards to Arsenal during the 2021-2022 season?\nRefereeing decisions play huge outcome in a football match. In fact, fans and analysts alike do pay enormous attention to the designated referee for each fixture. As a lifelong Arsenal fan, I wanted to find out which referees tend to be harsher on Arsenal.\n\n\n\n\n\n\n\n\n\nThe bar chart represents the total number of yellow cards each referee has given to Arsenal. Craig Pawson handed out the most yellow cards to Arsenal, with 9 cards given."
  },
  {
    "objectID": "Project_4.html",
    "href": "Project_4.html",
    "title": "Project 4: SQL",
    "section": "",
    "text": "Wideband acoustic immittance (WAI) Analysis\nOriginal Data Source\nBackground: The WAI database is a comprehensive online database for normative adult WAI measurements. This database is designed to facilitate data sharing and analysis among researchers in the field of audiology. As of July 1, 2019, the database encompasses measurements from 12 peer-reviewed studies, totaling 640 subjects and 914 normal middle ears. This results in 286,774 data points across various frequencies. The establishment of this WAI database represents a significant advancement in audiological research, offering a centralized resource for normative data and promoting collaborative efforts in the study of middle-ear pathologies.\nThe objective is to recreate Figure 1, which represents mean absorbance data from select 12 publications in the Wideband Acoustic Immittance (WAI) database, showing how absorbance varies with frequency across different studies.\nStarter Code to establish connection to the WAI database.\n\nlibrary(DBI)\nlibrary(RMariaDB)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ncon_wai &lt;- dbConnect(\n  MariaDB(),\n  host = \"scidb.smith.edu\",\n  user = \"waiuser\",\n  password = \"smith_waiDB\",\n  dbname = \"wai\"\n)\n\nMeasurements &lt;- tbl(con_wai, \"Measurements\")\nPI_Info &lt;- tbl(con_wai, \"PI_Info\")\nSubjects &lt;- tbl(con_wai, \"Subjects\")\n\nThis SQL code below is for processing and aggregating data from the WAI database. More specifically, this combines data from two tables, Measurements and PI_Info, to calculate and aggregate absorbance values for specific studies and prepare data for visualization.\nWe are: 1. Calculate Mean Absorbance: for each unique combination of study, instrument, frequency, and year. 2. Generates Legend Labels: constructs a label for visualization 3. Filters Data: restricts the query to 12 specific studies of interest. 4. Groups Data: organizes results by study, frequency, instrument, and year.\n\n\nSELECT \n    Measurements.Identifier, \n    PI_Info.AuthorsShortList, \n    Measurements.Instrument, \n    Measurements.Frequency, \n    AVG(Measurements.Absorbance) AS MeanAbsorbance, \n    CONCAT(\n        PI_Info.AuthorsShortList, ' (', PI_Info.Year, ') N=',\n        COUNT(DISTINCT CONCAT(Measurements.SubjectNumber, Measurements.Ear))\n    ) AS Legend_Label\nFROM Measurements\nJOIN PI_Info \n    ON Measurements.Identifier = PI_Info.Identifier\nWHERE Measurements.Identifier IN (\n    'Abur_2014', 'Feeney_2017', 'Groon_2015', 'Lewis_2015', \n    'Liu_2008', 'Rosowski_2012', 'Shahnaz_2006', 'Shaver_2013', \n    'Sun_2016', 'Voss_1994', 'Voss_2010', 'Werner_2010'\n)\nGROUP BY \n    Measurements.Identifier, \n    Measurements.Instrument, \n    PI_Info.AuthorsShortList, \n    Measurements.Frequency, \n    PI_Info.Year;\n\nData Visualization:\n\ndata$Frequency &lt;- as.numeric(data$Frequency)\n\n\ndata &lt;- data %&gt;%\n  filter(Frequency &gt;= 200)\n\n# Create the plot\nggplot(data, aes(x = Frequency, y = MeanAbsorbance, color = Legend_Label)) +\n  geom_line(size = 0.8) +\n  labs(\n    title = \"Mean absorbance from publications in WAI database\",\n    x = \"Frequency (Hz)\",\n    y = \"Mean Absorbance\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(\n    trans = \"log10\",\n    breaks = c(200, 400, 600, 800, 1000, 2000, 4000, 6000, 8000),\n    labels = c(\"200\", \"400\", \"600\", \"800\", \"1000\", \"2000\", \"4000\", \"6000\", \"8000\"),\n    limits = c(200, 8000)\n  ) +\n  scale_y_continuous(\n    limits = c(0, 1),\n    expand = c(0, 0)\n  )\n\n\n\n\n\n\n\n\nAbove is the a replicate of Figure 1. Y-axis (Mean Absorbance) represents the average proportion of sound energy absorbed by the middle ear at various frequencies. Higher values indicate greater absorption, while lower values suggest more reflection. X-axis (Frequency in Hz) represents the sound frequency (logarithmic scale) ranging from 200 Hz to 8000 Hz.\nEach line corresponds to a specific study in the WAI database, identified by the first author’s name, publication year, and the number of participants.\nIn essence, the graph provides a comparative view of absorbance data from multiple studies. It highlights similarities and differences in how different populations and systems respond to sound frequencies.\nMost studies show increasing absorbance from 200 Hz to approximately 1000-2000 Hz, peaking, and then gradually decreasing at higher frequencies. Variability in the data such as differences in peak values and slopes likely reflects variations in study populations, equipment, or methodologies.\nDeeper Dive into Feeney et al. (2017)\nI decided to choose the most recent study among the 12 selected publications, which was Feeney et al. (2017). This contains various grouping variables such as age, sex, and race/ethnicity. I chose Sex as the grouping variable because it is a common demographic factor in audiological studies and is likely to show differences in middle-ear characteristics.\nThe below SQL query takes on a similar process as that of recreating Figure 1. In this case, the Identifier would be the Feeney_2017, instead of all the 12 studies.\n\n\nSELECT \n    Measurements.Frequency, \n    Subjects.Sex,\n    AVG(Measurements.Absorbance) AS MeanAbsorbance\nFROM Measurements\nJOIN Subjects\n    ON Measurements.SubjectNumber = Subjects.SubjectNumber\nWHERE Measurements.Identifier = 'Feeney_2017'\nGROUP BY \n    Measurements.Frequency, \n    Subjects.Sex\nORDER BY Measurements.Frequency;\n\nData Visualization:\nI maintained a similar format for the x and y axis. The X-axis represents frequency (logarithmic scale), the Y-axis shows mean absorbance, and lines differentiate groups by sex (Male, Female, Unknown).\n\ndata$Frequency &lt;- as.numeric(data$Frequency)\n\nggplot(data, aes(x = Frequency, y = MeanAbsorbance, color = Sex)) +\n  geom_line(size = 0.8) +\n  labs(\n    title = \"Mean Absorbance by Sex Across Frequencies (Feeney et al., 2017)\",\n    x = \"Frequency (Hz)\",\n    y = \"Mean Absorbance\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(\n    trans = \"log10\",\n    breaks = c(200, 400, 600, 800, 1000, 2000, 4000, 6000, 8000),\n    labels = c(\"200\", \"400\", \"600\", \"800\", \"1000\", \"2000\", \"4000\", \"6000\", \"8000\"),\n    limits = c(200, 8000)\n  ) +\n  scale_y_continuous(\n    limits = c(0, 1),\n    expand = c(0, 0)\n  )\n\n\n\n\n\n\n\n\nThis graph illustrates how middle-ear absorbance varies across frequencies for male, female, and unknown sex groups in the Feeney et al. (2017) study. While the general trend of absorbance is consistent across groups, slight variations at the extremes of the frequency range (low and high) may warrant further exploration. The similarity across sexes suggests that sex is not a significant factor influencing WAI in this study.\nSome key observations: 1. Absorbance increases as frequency rises from 200 Hz to around 1000-2000 Hz, peaks, and then decreases for higher frequencies. All groups show peak absorbance in the range of 1000-4000 Hz, a typical finding in WAI studies, as this range represents optimal middle-ear energy absorption. 2. The absorbance patterns for male and female groups are very similar across frequencies, suggesting that sex may have minimal impact on WAI results in this study. The “Unknown” group also follows a similar trajectory, possibly due to overlapping populations. 3. Slight differences can be seen at lower frequencies (200-400 Hz) and higher frequencies (&gt;6000 Hz), but these differences are marginal.\nBest practice purposes…\n\ndbDisconnect(con_wai)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Taehwan (Jack) Kim | 김태환",
    "section": "",
    "text": "Example Heading\nThis is regular text.\nThis is custom-sized text.\nMore regular text here."
  }
]