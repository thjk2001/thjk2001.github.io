[
  {
    "objectID": "case_study_1.html",
    "href": "case_study_1.html",
    "title": "Success in Olympics",
    "section": "",
    "text": "TidyTuesday Reference\nOriginal Data Source\nObjective: Usually it is the Olympic athletes that receive the fame. However, at the end of the day they each represent their country with pride. Let’s find out which nations are represented the most in the top 10 most successful athlete list (measured by medal count). So, which countries have produced the best talents?\nThis analysis utilizes a public dataset on Kaggle compiled by RGriffen. This is a historical dataset on the modern Olympic Games, including all the Games from Athens 1896 to Rio 2016. It was compiled from scrapping data from Sport Reference on May 2018.\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-08-06')\n\nolympics &lt;- tuesdata$olympics\n\nOriginal Data\n\nlibrary(DT)\n\n# Display the first 5 rows as an interactive table\ndatatable(\n  olympics[1:5, ],  # Select the first 5 rows\n  options = list(\n    pageLength = 5,      # Show 5 rows\n    scrollX = TRUE,      # Enable horizontal scrolling\n    dom = 't',           # Show only the table (no search box, etc.)\n    autoWidth = TRUE     # Adjust column width automatically\n  ),\n  caption = \"First 5 Rows of the Olympics Dataset\"\n)\n\n\n\n\n\nAbove is a sample table showing the first 5 rows of Olympics dataset. This data includes all athletes who participated in past Olympics. Team represents the country each athlete represented and the variable “noc” represents the three-letter abbreviation. Some columns do have missing variable. For example, some athletes may have missing data on their height and weight. If the medal column is missing, it means that the player did not win a medal during his/her participation of that particular event. If a player did win a medal, it will be indicated as Bronze, Silver or Gold.\n\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# Treating Soviet Union and Russia as one country and representing it as USSR/Russia\nolympics &lt;- olympics |&gt;\n  mutate(team = case_when(\n    team %in% c(\"Soviet Union\", \"Russia\") ~ \"USSR/Russia\",\n    TRUE ~ team\n  ))\n\n#Obtaining best athletes\ntop_athletes &lt;- olympics |&gt;\n  filter(!is.na(medal)) |&gt;  # first filters for those only with medals\n  group_by(name, team) |&gt;\n  summarise(total_medals = n(), .groups = \"drop\") |&gt;\n  arrange(desc(total_medals)) |&gt;\n  slice(1:10)\n\n# Create a clean table for Top 10 Athletes\ntop_athletes |&gt;\n  kable(\n    caption = \"Top 10 Athletes by Total Medals (USSR/Russia Combined)\",\n    col.names = c(\"Name\", \"Team\", \"Total Medals\"),\n    align = \"lcl\"\n  ) |&gt;\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),\n    full_width = FALSE\n  )\n\n\nTop 10 Athletes by Total Medals (USSR/Russia Combined)\n\n\nName\nTeam\nTotal Medals\n\n\n\n\nMichael Fred Phelps, II\nUnited States\n28\n\n\nLarysa Semenivna Latynina (Diriy-)\nUSSR/Russia\n18\n\n\nNikolay Yefimovich Andrianov\nUSSR/Russia\n15\n\n\nBorys Anfiyanovych Shakhlin\nUSSR/Russia\n13\n\n\nEdoardo Mangiarotti\nItaly\n13\n\n\nOle Einar Bjrndalen\nNorway\n13\n\n\nTakashi Ono\nJapan\n13\n\n\nAleksey Yuryevich Nemov\nUSSR/Russia\n12\n\n\nDara Grace Torres (-Hoffman, -Minas)\nUnited States\n12\n\n\nJennifer Elisabeth \"Jenny\" Thompson (-Cumpelik)\nUnited States\n12\n\n\n\n\n\n\n# Filter by nations for the top athletes\ntop_nations &lt;- top_athletes |&gt;\n  group_by(team) |&gt;\n  summarise(count = n(), .groups = \"drop\") |&gt;\n  arrange(desc(count))\n\n# Create clean table for Top Nations\ntop_nations |&gt;\n  kable(\n    caption = \"Top Nations by Number of Top Athletes (USSR/Russia Combined)\",\n    col.names = c(\"Team\", \"Number of Athletes\"),\n    align = \"lc\"\n  ) |&gt;\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),\n    full_width = FALSE\n  )\n\n\nTop Nations by Number of Top Athletes (USSR/Russia Combined)\n\n\nTeam\nNumber of Athletes\n\n\n\n\nUSSR/Russia\n4\n\n\nUnited States\n3\n\n\nItaly\n1\n\n\nJapan\n1\n\n\nNorway\n1\n\n\n\n\n\n\n\nAs for most successful athletes, Michael Phelps by far has won the most medals (28). His success if unprecedented, with the second most decorated athlete having a whopping 10 fewer medals than Phelps.\n\nlibrary(ggplot2)\n\n# Creating a bar chart\nggplot(top_nations, aes(x = reorder(team, -count), y = count)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(title = \"Number of Top 10 Athletes by Country\",\n       x = \"Country\",\n       y = \"Number of Athletes\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nRussia (formerly USSR) produced the highest number of most successful athletes. Russia/USSR and the US usually rank top 2 for medal counts. It is actually surprising to find China not represented in the list, given the recent success of China in Olympics."
  },
  {
    "objectID": "case_study_3.html",
    "href": "case_study_3.html",
    "title": "Netflix Movies",
    "section": "",
    "text": "TidyTuesday Reference\nOriginal Data Source\nObjective: Analyze some interesting trends in Netflix movies as of 2019\nThis dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search engine.\nIn 2018, Flixable released an interesting report which shows that the number of TV shows on Netflix has nearly tripled since 2010. The streaming service’s number of movies has decreased by more than 2,000 titles since 2010, while its number of TV shows has nearly tripled. It will be interesting to explore what other insights can be obtained from the same dataset.\nIntegrating this dataset with other external datasets such as IMDB ratings, rotten tomatoes can also provide many interesting findings.\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-04-20')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 17)\n\nnetflix &lt;- tuesdata$netflix\n\nlibrary(tidyverse)\n\nOriginal Data\n\nlibrary(DT)\n\n# Display the first 5 rows of the netflix dataset\ndatatable(\n  netflix[1:5, ],  # Select the first 5 rows\n  options = list(\n    pageLength = 5,      # Show 5 rows\n    scrollX = TRUE,      # Enable horizontal scrolling\n    dom = 't',           # Show only the table without search box, etc.\n    autoWidth = TRUE     # Adjust column widths automatically\n  ),\n  caption = \"First 5 Rows of the Netflix Dataset\"\n)\n\n\n\n\n\nThe above sample table shows the first five rows of the original data. Descriptions of the variables are the following:\n\nshow_id: Unique ID for every Movie / Tv Show\ntype: Identifier - A Movie or TV Show\ntitle: Title of the Movie / Tv Show\ndirector: Director of the Movie/Show\ncast: Actors involved in the movie / show\ncountry: Country where the movie / show was produced\ndate_added: Date it was added on Netflix\nrelease_year: Actual Release year of the movie / show\nrating: TV Rating of the movie / show\nduration: Total Duration - in minutes or number of seasons\nlisted_in: Genre\ndescription: Summary description of the film/show\n\n\n\n\n# We want to look at actors for movies\nactor_count &lt;- netflix |&gt;\n  filter(type == \"Movie\", !is.na(cast)) |&gt; \n  separate_rows(cast, sep = \",\\\\s*\") |&gt; \n  count(cast, sort = TRUE)\n\n# Top 5 actors\ntop_5_actors &lt;- actor_count |&gt;\n  slice_head(n = 5) \n\n# Creating a bar chart but having the names on the vertical axis since they are long\nggplot(top_5_actors, aes(x = reorder(cast, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +  \n  labs(\n    title = \"Top 5 Actors Appearing in Netflix Titles\",\n    x = \"Actor\",\n    y = \"Number of Appearances\"\n  ) +\n  theme_minimal() \n\n\n\n\n\n\n\n\nInteresting, I actually do not know any of these actors/actresses. Upon some research, I have found that all of them are Indian actors/actresses. These actors/actresses were very active in their respective careers.\nAs a side analysis, let’s find out which countries produced the most number of movies?\n\n# Counting the movies by the countru they were produced\ncountry_count_movies &lt;- netflix |&gt;\n  filter(type == \"Movie\", !is.na(country)) |&gt;  \n  separate_rows(country, sep = \",\\\\s*\") |&gt;  \n  count(country, sort = TRUE)  \n\n# Finding the top 5 countries\ntop_5_countries &lt;- country_count_movies |&gt;\n  slice_head(n = 5)  \n\n\nggplot(top_5_countries, aes(x = reorder(country, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +  \n  labs(\n    title = \"Top 5 Countries Producing the Most Netflix Movies\",\n    x = \"Country\",\n    y = \"Number of Movies\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nUnsurprisingly, the US has produced the most Netflix movies. This makes sense given that Hollywood is the global hub of film production. The US has produced more than double the amount of movies than second place India and more than the movie productions of the next four countries combined. With the growing size of Bollywood, India comes in for second place. This may speak to the high number of Indian actors/actresses in the first study.\n\n\n\nI have always wondered which words are commonly used in movie titles. We have excluded articles, pronouns, prepositions and any common stop words.\n\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(stringr)\n\n# A list of words we want to exclude\nstop_words &lt;- c(\"a\", \"an\", \"the\", \"and\", \"in\",\n                \"on\", \"of\", \"with\", \"for\", \"to\",\n                \"he\", \"she\", \"my\", \"it\", \"him\",\n                \"her\", \"above\", \"below\", \"top\",\n                \"bottom\", \"between\", \"front\", \"back\",\n                \"beneath\", \"they\", \"me\", \"you\", \"them\",\n                \"us\", \"we\", \"that\", \"this\", \"these\",\n                \"those\", \"I\", \"from\", \"i\", \"at\", \"\")\n\n# Filter and preprocess the titles\nword_count &lt;- netflix |&gt;\n  subset(type == \"Movie\" & !is.na(title)) |&gt;\n  transform(clean_title = str_remove_all(title, \"[^A-Za-z\\\\s]\")) |&gt;\n  transform(clean_title = tolower(clean_title)) |&gt;\n  {\\(df) data.frame(clean_title = unlist(strsplit(df$clean_title, \"\\\\s+\")))}() |&gt;\n  subset(!clean_title %in% stop_words) |&gt;\n  table() |&gt;\n  as.data.frame()\n\n# Sort and rename columns\nword_count &lt;- word_count[order(-word_count$Freq), ]\ncolnames(word_count) &lt;- c(\"Word\", \"Frequency\")\n\n# Display top 10 words as a formatted table\nword_count |&gt;\n  head(10) |&gt;\n  kable(\n    caption = \"&lt;strong style='color:black; font-size:16px;'&gt;Top 10 Most Frequent Words in Netflix Movie Titles&lt;/strong&gt;\",\n    col.names = c(\"Word\", \"Frequency\"),\n    row.names = FALSE,  # Suppress row numbers\n    align = c(\"l\", \"c\"),\n    escape = FALSE\n  ) |&gt;\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),\n    full_width = FALSE,\n    position = \"center\"\n  )\n\n\nTop 10 Most Frequent Words in Netflix Movie Titles\n\n\nWord\nFrequency\n\n\n\n\nlove\n91\n\n\nchristmas\n72\n\n\nmovie\n60\n\n\nman\n58\n\n\nstory\n54\n\n\nlife\n44\n\n\nworld\n42\n\n\nlittle\n39\n\n\nlive\n38\n\n\none\n38\n\n\n\n\n\n\n\nLove and Christmas are the top two most common words. These results do makes sense as these are popular topics.\n\n\n\nThe most commercially successful filmmaker in the history of Hollywood is Steven Spielberg. Steven Spielberg is the creator of many of the most successful franchises and stand-alone films. They include the Indiana Jones series, the Jurassic Park series, Saving Private Ryan, and Jaws. Let’s figure out which of his movies are on Netflix.\n\n# Filter for Spielberg movies\nspielberg_movies &lt;- netflix |&gt;\n  filter(type == \"Movie\", str_detect(director, \"Steven Spielberg\"))\n\nspielberg_movies |&gt;\n  select(title) |&gt;  # Select only the title column\n  rename(\"Title\" = title) |&gt;  # Rename the column for clarity\n  kable(\n    caption = \"&lt;strong style='color:black; font-size:16px;'&gt;Steven Spielberg Movies Available on Netflix&lt;/strong&gt;\",\n    col.names = c(\"Title\"),\n    align = \"l\",\n    escape = FALSE \n  ) |&gt;\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),\n    full_width = FALSE,\n    position = \"center\"\n  )\n\n\nSteven Spielberg Movies Available on Netflix\n\n\nTitle\n\n\n\n\nCatch Me If You Can\n\n\nHook\n\n\nIndiana Jones and the Kingdom of the Crystal Skull\n\n\nIndiana Jones and the Last Crusade\n\n\nIndiana Jones and the Raiders of the Lost Ark\n\n\nIndiana Jones and the Temple of Doom\n\n\nLincoln\n\n\nSchindler's List\n\n\nThe Adventures of Tintin\n\n\nWar Horse\n\n\n\n\n\n\n\nSurprising, only 10 are available on Netflix. Given sheer amount of Spielberg movies we know of, this a very small number."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Taehwan (Jack) Kim",
    "section": "",
    "text": "Greetings! I’m Jack. Junior and Economics major at Pomona College. Formerly of Class of 2024, finally back at school after serving in the Republic of Korea Army. My interest lies in the world of finance and value investing. Love to play soccer (lifelong Arsenal fan) and love to share my military endeavors. This is a space to test my analytical skillset."
  },
  {
    "objectID": "Project_2.html",
    "href": "Project_2.html",
    "title": "Project_2",
    "section": "",
    "text": "TidyTuesday Reference\nOriginal Data Source\nThis dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search engine.\nIn 2018, they released an interesting report which shows that the number of TV shows on Netflix has nearly tripled since 2010. The streaming service’s number of movies has decreased by more than 2,000 titles since 2010, while its number of TV shows has nearly tripled. It will be interesting to explore what all other insights can be obtained from the same dataset.\nIntegrating this dataset with other external datasets such as IMDB ratings, rotten tomatoes can also provide many interesting findings.\n\n# Get the Data\n\n# Read in with tidytuesdayR package \n# Install from CRAN via: install.packages(\"tidytuesdayR\")\n# This loads the readme and all the datasets for the week of interest\n\n# Either ISO-8601 date or year/week works!\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-04-20')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 17)\n\nnetflix &lt;- tuesdata$netflix\n\nlibrary(tidyverse)\n\n\nWho are the actors with the most movie appearances? Let’s find the top 5 actors by the number of appearances.\n\n\nactor_count &lt;- netflix |&gt;\n  filter(type == \"Movie\", !is.na(cast)) |&gt; \n  separate_rows(cast, sep = \",\\\\s*\") |&gt; \n  count(cast, sort = TRUE)\n\ntop_5_actors &lt;- actor_count |&gt;\n  slice_head(n = 5) \n\n\nggplot(top_5_actors, aes(x = reorder(cast, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +  \n  labs(\n    title = \"Top 5 Actors Appearing in Netflix Titles\",\n    x = \"Actor\",\n    y = \"Number of Appearances\"\n  ) +\n  theme_minimal() \n\n\n\n\n\n\n\n\nInteresting, I actually do not know any of these actors/actresses. Upon some research, I have found that all of them are Indian actors/actresses.\nJust out of curiosity, let’s find out which countries produced the most number of movies?\n\ncountry_count_movies &lt;- netflix |&gt;\n  filter(type == \"Movie\", !is.na(country)) |&gt;  \n  separate_rows(country, sep = \",\\\\s*\") |&gt;  \n  count(country, sort = TRUE)  \n\ntop_5_countries &lt;- country_count_movies |&gt;\n  slice_head(n = 5)  \n\n\nggplot(top_5_countries, aes(x = reorder(country, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +  \n  labs(\n    title = \"Top 5 Countries Producing the Most Netflix Movies\",\n    x = \"Country\",\n    y = \"Number of Movies\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nUnsurprisingly, the US has produced the most Netflix movies.This makes sense given that Hollywood is the global hub of film production. With the growing size of Bollywood, India comes in for second place.\n\nMost popular words in movie titles:\n\nWe have excluded articles, pronouns, prepositions and any common stop words.\n\n# A list of words we want to exclude\nstop_words &lt;- c(\"a\", \"an\", \"the\", \"and\", \"in\", \"on\", \"of\", \"with\", \"for\", \"to\", \"he\", \"she\", \"my\", \"it\", \"him\", \"her\", \"above\", \"below\", \"top\", \"bottom\", \"between\", \"front\", \"back\", \"beneath\", \"they\", \"me\", \"you\", \"them\", \"us\", \"we\", \"that\", \"this\", \"these\", \"those\", \"I\", \"from\", \"i\", \"at\", \"\")\n\nword_count &lt;- netflix |&gt;\n  filter(type == \"Movie\", !is.na(title)) |&gt;\n  mutate(clean_title = str_remove_all(title, \"[^A-Za-z\\\\s]\")) |&gt;\n  mutate(clean_title = str_to_lower(clean_title)) |&gt;\n  separate_rows(clean_title, sep = \"\\\\s+\") |&gt; \n  filter(!clean_title %in% stop_words) |&gt; \n  count(clean_title, sort = TRUE) \n\nhead(word_count, 10)\n\n# A tibble: 10 × 2\n   clean_title     n\n   &lt;chr&gt;       &lt;int&gt;\n 1 love           91\n 2 christmas      72\n 3 movie          60\n 4 man            58\n 5 story          54\n 6 life           44\n 7 world          42\n 8 little         39\n 9 live           38\n10 one            38\n\n\nLove and Christmas are the top two most common words. These results do makes sense as these are pretty popular topics.\n\nThe most commercially successful filmmaker in the history of Hollywood is Steven Spielburg. Let’s figure out which of his movies are on Netflix.\n\n\nspielberg_movies &lt;- netflix |&gt;\n  filter(type == \"Movie\", str_detect(director, \"Steven Spielberg\"))\n\nhead(spielberg_movies$title)\n\n[1] \"Catch Me If You Can\"                               \n[2] \"Hook\"                                              \n[3] \"Indiana Jones and the Kingdom of the Crystal Skull\"\n[4] \"Indiana Jones and the Last Crusade\"                \n[5] \"Indiana Jones and the Raiders of the Lost Ark\"     \n[6] \"Indiana Jones and the Temple of Doom\"              \n\n\nSurprising, only 6 are available on Netflix. Given sheer amount of Spielberg movies we know of, this a very small number.\n\nFollowing Covid-19, less people are going to the movies. At the same time over-the-top streaming services have become more influential. Nowadays, people are wondering “when is my favorite movie going to be on Netflix?”. So let’s find out, on average, how long does it take for a movie to be added on Netflix.\n\nTwo caveats: - We have exact dates for the when the movie was added, but only the year data is available for when it was released. Hence, we will just find the number of years. - Netflix started streaming services in 2007. Any movie produced before 2007 will be excluded because that just automatically increases the average.\n\nnetflix &lt;- netflix |&gt;\n  mutate(\n    date_added = mdy(date_added),  \n    release_year = as.numeric(release_year)  \n  )\n\nnetflix &lt;- netflix |&gt;\n  mutate(added_year = year(date_added))  \n\nnetflix &lt;- netflix |&gt;\n  mutate(time_diff_years = added_year - release_year)  \n\nnetflix_era &lt;- netflix |&gt;\n  filter(!is.na(time_diff_years) & time_diff_years &gt;= 0 & release_year &gt;= 2007) \n\naverage_time_diff &lt;- netflix_era |&gt;\n  summarize(average_diff = mean(time_diff_years, na.rm = TRUE))\n\naverage_time_diff\n\n# A tibble: 1 × 1\n  average_diff\n         &lt;dbl&gt;\n1         2.14\n\n\nOn average, it takes 2.14 years for a movie to be added on Netflix after being released in the theaters."
  },
  {
    "objectID": "Project_3.html",
    "href": "Project_3.html",
    "title": "NYC Flights Analysis",
    "section": "",
    "text": "Objective: In this analysis, I will investigate whether there is a significant difference in departure delays between morning and evening flights using data from the nycflights13 package. This data set contains all the relevant information of a particular flight schedule. To perform this analysis, I will perform a permutation test, where I will shuffle the time-of-day labels (morning/evening) and repeatedly calculate the difference in mean departure delays. This will help simulate the null hypothesis, which assumes that the time of day has no statistical significance on departure delays. By comparing the observed difference in delays to this simulated distribution, I will assess the statistical significance of the observed difference.\n[Null Hypothesis] There is no difference in the mean departure delays between morning flights and evening flights.\n\nlibrary(nycflights13)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(DT)\n\n\n# Create a new table from original to add a new column indicator morning or evening flight\nflights &lt;- nycflights13::flights |&gt;\n  filter(!is.na(dep_delay)) |&gt;\n  mutate(time_of_day = case_when(\n    hour &gt;= 5 & hour &lt; 11 ~ \"morning\",\n    hour &gt;= 17 & hour &lt; 23 ~ \"evening\",\n    TRUE ~ \"other\"\n  )) |&gt;\n  filter(time_of_day != \"other\")  # Exclude flights that are not in the morning or evening\n\n\n# Display the first 5 rows as an interactive table\ndatatable(\n  flights[1:5, ],  # Select the first 5 rows\n  options = list(\n    pageLength = 5,      # Display 5 rows per page\n    dom = 't',           # Show only the table (no search box, etc.)\n    scrollX = TRUE       # Enable horizontal scrolling\n  ),\n  caption = \"First 5 Rows of Flights Data\"\n)\n\n\n\n\n\nWe are using the nycflights13 data which contains the flight information in New York airports in 2013. The above sample table includes all the relevant information for each flight schedule and also has a new column called time_of_day as an indicator for whether the flight is in the morning or in the evening.\nMost importantly, the data shows the departure delay time (in minutes). A positive value means the flight was delayed. A negative value means the flight departed earlier than planned schedule. A value of zero indicates the flight departed on time. We will first define morning and evening flights. Morning flights are flights departing between 5:00am and 11:00am. Evening flights are flights departing from 5:00pm and 11:00pm (ex. First row, dep_time 517 means departure time is at 5:17am). We removed the flights that are not within these time ranges.\nLet’s first check how many morning and evening flights there are, respectively.\n\n# Count the number of flights for each time_of_day\nflight_counts &lt;- flights |&gt;\n  group_by(time_of_day) |&gt;\n  summarize(flight_count = n())\n\n# Create a bar chart\nggplot(flight_counts, aes(x = time_of_day, y = flight_count, fill = time_of_day)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = flight_count), vjust = -0.5) +\n  labs(\n    title = \"Number of Flights by Time of Day\",\n    x = \"Time of Day\",\n    y = \"Number of Flights\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nThere are more morning flights than evening flights. Moreover, the difference is noticeable, with about 20,000 more flights classified as morning.\nBelow we have the average delayed time for morning and evening flights.\n\nlibrary(ggplot2)\n\n# Calculate mean delays\nmean_delays &lt;- flights |&gt;\n  group_by(time_of_day) |&gt;\n  summarize(mean_delay = mean(dep_delay, na.rm = TRUE))\n\n# Create the bar chart\nggplot(mean_delays, aes(x = time_of_day, y = mean_delay, fill = time_of_day)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = round(mean_delay, 2)), vjust = -0.5) +\n  labs(\n    title = \"Mean Departure Delay by Time of Day\",\n    x = \"Time of Day\",\n    y = \"Mean Delay (minutes)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nThen we calculate the observed differences.\n\nobs_diff &lt;- flights |&gt;\n  group_by(time_of_day) |&gt;\n  summarize(mean_delay = mean(dep_delay)) |&gt;\n  summarize(diff = diff(mean_delay)) |&gt;\n  pull(diff)\n\n\n\nObserved Difference in Mean Delays (Morning - Evening): -19.23765 minutes\n\n\n[Permutation Test] We’ll shuffle the time labels (morning or evening) 1000 times and calculate the difference in mean delays for each permutation.\n\nset.seed(47)\n\nnull_dist &lt;- map_dbl(1:1000, ~ {\n  flights |&gt; \n    mutate(shuffled_time = sample(time_of_day)) |&gt; \n    group_by(shuffled_time) |&gt; \n    summarize(mean_delay = mean(dep_delay), .groups = \"drop\") |&gt; \n    summarize(diff = diff(mean_delay)) |&gt; \n    pull(diff)\n})\n\nLet’s plot the null distribution with observed difference\n\nggplot(data.frame(null_dist), aes(x = null_dist)) +\n  geom_histogram(binwidth = 0.5, fill = \"lightblue\", color = \"black\") +\n  geom_vline(xintercept = obs_diff, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Permutation Test: Morning vs Evening Delays\",\n    x = \"Difference in Mean Delays\",\n    y = \"Frequency\"\n  )\n\n\n\n\n\n\n\n\n\n# Calculate p-value\np_value &lt;- mean(abs(null_dist) &gt;= abs(obs_diff))\n\n\n\nP-value: 0 \n\n\nResults\nThe red dashed line represents the observed difference in mean delays between morning and evening flights. The histogram represents the differences in mean delays generated under the null hypothesis (shuffling the time labels).\nA p-value of 0 means that none of the 1000 permutations generated a difference in mean delays as extreme (or more extreme) as the observed difference. This indicates that the observed difference is highly unlikely to have occurred by chance if the null hypothesis were true.\nSince the p-value is 0, this provides strong evidence against the null hypothesis. You would reject the null hypothesis and conclude that the time of day (morning vs. evening) may be correlated to departure delays.\nThe visualization supports this as the red line falls outside the range of the simulated null distribution values, highlighting that the observed difference is not consistent with the null hypothesis assumption.\nGreater Insights\nThis study provides strong evidence for the correlation between time of day and average departure delay time. According to the analysis, evening flights have nearly a 20 minute longer delays than morning flights. This is interesting given the fact that there were significantly more morning flights than evening flights. While more number of flights does not automatically indicate more flight traffic, the results are interesting.\nJust to dive, some potential reasons could explain the longer delays for evening flights. Below are some factors to consider:\n\nCumulative Delays Throughout the Day: Flights in the evening are often impacted by delays that build up over the day (e.g., late arrivals, extended turnaround times).\nWeather Conditions: Thunderstorms and other adverse weather conditions are more common in the late afternoon or early evening, especially in summer months.\nAir Traffic Congestion: Evening hours are often a peak time for flights returning from major business or leisure destinations, leading to increased congestion in the air and at the airport.\nCrew and Aircraft Rotation: If an aircraft or crew is late arriving from an earlier flight, it can cascade into delays for subsequent evening flights.\nRunway Closures or Maintenance: Some airports schedule runway or terminal maintenance for late evenings, which can cause operational disruptions.\n\nTo find some empirical reasons directly from the data, let’s look at early departure flights (those that have negative values for dep_delay column). The observed means of departure delays that we calculated in the above analysis takes into account all flights, regardless of whether their status was on time, early departure, or delayed departure. If either morning or evening flights have a significant number of early flights, then, that may potentially have affected the averages.\n\n# Filter flights with dep_delay &lt; 0 and group by time_of_day\nearly_departures &lt;- flights |&gt;\n  filter(dep_delay &lt; 0) |&gt;\n  group_by(time_of_day) |&gt;\n  summarize(early_flights = n())\n\n# Create a bar chart with adjusted limits and label position\nggplot(early_departures, aes(x = time_of_day, y = early_flights, fill = time_of_day)) +\n  geom_bar(stat = \"identity\", width = 0.6) +\n  geom_text(aes(label = early_flights), vjust = -0.2, size = 5) +\n  labs(\n    title = \"Number of Flights Departing Earlier Than Scheduled by Time of Day\",\n    x = \"Time of Day\",\n    y = \"Number of Early Departures\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n    axis.title.x = element_text(size = 12),\n    axis.title.y = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  ) +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.05)))\n\n\n\n\n\n\n\n\nLooking at the results, there are significantly more morning flights that departed earlier than scheduled. This has contributed to the significantly lower average departure delay for morning flights. This offers some empirical evidence based on our database as to why our calculated averages have a significant difference.\nWhile this particular caveat lends way to the argument for calculating the departure delay averages for those flights that were actually delayed, I believe this misleads the purpose of the study. You do not know whether your flight is going to be departing earlier than planned or delayed. We all expect our flights to depart on time. Therefore, simply calculating the averages for those flights that do actually get delayed misses this initial thought process. One particular flight can be delayed for some reason A. Well, that same reason A can be the basis for an early flight departure of another flight. Hence, I still believe it is more appropriate to examine all flights.\nSo how can we interpret these results in a broader context? The current analysis is based on flights departing from NYC airports (JFK, LGA, EWR) in 2013 and data acquired under the assumption that morning and evening flights do not have missing delay information. This means the conclusion of a significant difference in mean delays applies only to NYC flights in 2013.\nFor potential larger populations, we could generalize the results to other years in NYC. Similar patterns of delays (morning flights being less delayed than evening flights) might hold for other years if no major changes occurred in scheduling, infrastructure, or air traffic. It may also be possible to generalize to similar metropolitan hubs. NYC is one of the busiest air travel hubs in the US, so patterns observed in NYC might apply to other large cities with comparable air traffic and flight schedules, such as Chicago (ORD, MDW), Los Angeles (LAX), or Atlanta (ATL). However, generalization should be done cautiously and supported by additional evidence. Moreover, there may be limitations to generalization in the sense that regional factors may come into play. NYC has unique air traffic patterns due to its size, location, and volume of international and domestic flights. Other cities may not share these characteristics."
  },
  {
    "objectID": "case_study_2.html",
    "href": "case_study_2.html",
    "title": "Arsenal in Premier League 2021-2022",
    "section": "",
    "text": "TidyTuesday Reference\nOriginal Data Source\nObjective: Which referees gave out the most yellow cards to Arsenal during the 2021-2022 season?\nThe dataset includes teams, referee, and stats by home and away side such as fouls, shots, cards, and many more. This dataset was compiled from the official Premier League website for the 2021-2022 season. This data was scrapped by Evan Gower.\n\nlibrary(tidytuesdayR)\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-04-04')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 14)\n\nsoccer &lt;- tuesdata$soccer\n\nOriginal Data\n\nlibrary(DT)\n\n# Display the first 5 rows of the soccer dataset\ndatatable(\n  soccer[1:5, ],  # Select the first 5 rows\n  options = list(\n    pageLength = 5,      # Show 5 rows\n    scrollX = TRUE,      # Enable horizontal scrolling\n    dom = 't',           # Show only the table without search box, etc.\n    autoWidth = TRUE     # Adjust column widths automatically\n  ),\n  caption = \"First 5 Rows of the Soccer Dataset\"\n)\n\n\n\n\n\nThe above sample table shows the first five rows of the original dataset. Each row represents a unique matchday for a given team. Descriptions of some key variables are the following:\n\nFTHG: full time home team goals\nFTAG: full time away team goals\nFTR: results showing which team won\nHalf-time results are denoted by HTHG, HTAG, HTR\nHS: Number of shots taken by the home team\nAS: Number of shots taken by the away team\nHST: Number of shots on target by the home team\nAST: Number of shots on target by the away team\nHF: Number of fouls by the home team\nAF: Number of fouls by the away team\nHC: Number of corners taken by the home team\nAC: Number of corners taken by the away team\nHY: Number of yellow cards received by the home team\nAY: Number of yellow cards received by the away team\nHR: Number of red cards received by the home team\nAR: Number of red cards received by the away team\n\nRefereeing decisions play huge outcome in a football match. In fact, fans and analysts alike do pay enormous attention to the designated referee for each fixture. As a lifelong Arsenal fan, I wanted to find out which referees tend to be harsher on Arsenal. More specifically, we focus on yellow cards. While a red card means that a player is ejected from the match, these instances tend to be more clear-cut decisions. Yellow card decisions offer a better insight into the deeper nuances of refereeing decisions or potential biases toward/against teams since yellow cards are given to less aggressive actions.\n\nlibrary(tidyverse)\n\n# Creating a comprehensive table for Disciplinary actions data\narsenal_yellow_by_referee &lt;- soccer |&gt;\n  filter(HomeTeam == \"Arsenal\" | AwayTeam == \"Arsenal\") |&gt;\n  group_by(Referee) |&gt;\n  summarise(\n    Home_Yellow_Cards = sum(HY[HomeTeam == \"Arsenal\"], na.rm = TRUE),\n    Away_Yellow_Cards = sum(AY[AwayTeam == \"Arsenal\"], na.rm = TRUE),\n    Total_Yellow_Cards = Home_Yellow_Cards + Away_Yellow_Cards\n  ) |&gt;\n  arrange(desc(Total_Yellow_Cards))\n\n\n\n\nlibrary(ggplot2)\n\n# Bar char to represent yellow cards by referees\nggplot(arsenal_yellow_by_referee, aes(x = reorder(Referee, -Total_Yellow_Cards), y = Total_Yellow_Cards)) +\n  geom_bar(stat = \"identity\", fill = \"yellow\") +\n  geom_text(aes(label = Total_Yellow_Cards), vjust = -0.5, size = 4) +\n  labs(title = \"Yellow Cards Arsenal Received by Referees (2021-2022 Season)\", \n       x = \"Referee\", \n       y = \"Total Yellow Cards\") +\n  scale_y_continuous(breaks = NULL) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1, size = 10), \n    axis.text.y = element_blank(),  \n    axis.ticks.y = element_blank(), \n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank()  \n  )\n\n\n\n\n\n\n\n\nThe bar chart represents the total number of yellow cards each referee has given to Arsenal. Craig Pawson handed out the most yellow cards to Arsenal, with 9 cards given.\n\n\n\n\nlibrary(ggplot2)\n\n# Summarize total yellow cards at home and away\nyellow_cards_summary &lt;- arsenal_yellow_by_referee %&gt;%\n  summarise(\n    Home = sum(Home_Yellow_Cards),\n    Away = sum(Away_Yellow_Cards)\n  ) |&gt;\n  pivot_longer(cols = everything(), names_to = \"Location\", values_to = \"Total_Yellow_Cards\")\n\n\nggplot(yellow_cards_summary, aes(x = Location, y = Total_Yellow_Cards, fill = Location)) +\n  geom_bar(stat = \"identity\", width = 0.6) +\n  geom_text(aes(label = Total_Yellow_Cards), vjust = -0.2, size = 5) + \n  labs(\n    title = \"Yellow Cards Arsenal Received by Location\",\n    x = \"Location\",\n    y = \"Total Yellow Cards\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.margin = margin(10, 10, 20, 10), \n    text = element_text(size = 14),\n    legend.position = \"none\"\n  ) +\n  ylim(0, max(yellow_cards_summary$Total_Yellow_Cards) + 5)\n\n\n\n\n\n\n\n\nThis study offers insight into if Arsenal experienced home advantage in referee decisions. Many fans assume that referees tend to be more lenient toward home teams. This may manifest the most in referee decisions for making disciplinary actions. The bar chart shows the number of yellow cards Arsenal received depending on match venue.\nInterestingly, Arsenal received 29 yellow cards at away matches and 31 in home matches. This goes against common logic. Although not conclusive, home advantage did not work in Arsenal’s favor."
  },
  {
    "objectID": "Project_4.html",
    "href": "Project_4.html",
    "title": "Wideband Acoustic Immittance Analysis using SQL",
    "section": "",
    "text": "Objective: Replicating WAI Analysis and Examine a Single Study\nOriginal Data Source\nBackground: The WAI database is a comprehensive online database for normative adult WAI measurements. This database is designed to facilitate data sharing and analysis among researchers in the field of audiology. As of July 1, 2019, the database encompasses measurements from 12 peer-reviewed studies, totaling 640 subjects and 914 normal middle ears. This results in 286,774 data points across various frequencies. The establishment of this WAI database represents a significant advancement in audiological research, offering a centralized resource for normative data and promoting collaborative efforts in the study of middle-ear pathologies.\n\n\nThe objective is to recreate Figure 1, which represents mean absorbance data from select 12 publications in the Wideband Acoustic Immittance (WAI) database, showing how absorbance varies with frequency across different studies.\nStarter Code to establish connection to the WAI database.\n\nlibrary(DBI)\nlibrary(RMariaDB)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ncon_wai &lt;- dbConnect(\n  MariaDB(),\n  host = \"scidb.smith.edu\",\n  user = \"waiuser\",\n  password = \"smith_waiDB\",\n  dbname = \"wai\"\n)\n\nMeasurements &lt;- tbl(con_wai, \"Measurements\")\nPI_Info &lt;- tbl(con_wai, \"PI_Info\")\nSubjects &lt;- tbl(con_wai, \"Subjects\")\n\nThis SQL code below is for processing and aggregating data from the WAI database. More specifically, this combines data from two tables, Measurements and PI_Info, to calculate and aggregate absorbance values for specific studies and prepare data for visualization.\nThe following is the detailed process:\n\nCalculate Mean Absorbance: for each unique combination of study, instrument, frequency, and year.\nGenerates Legend Labels: constructs a label for visualization\nFilters Data: restricts the query to 12 specific studies of interest.\nGroups Data: organizes results by study, frequency, instrument, and year.\n\n\n-- Select the relevant columns and compute aggregated values\nSELECT \n    Measurements.Identifier, \n    PI_Info.AuthorsShortList, \n    Measurements.Instrument, \n    Measurements.Frequency, \n    AVG(Measurements.Absorbance) AS MeanAbsorbance, \n-- Create a descriptive label combining author names, year, and sample size    \n    CONCAT(\n        PI_Info.AuthorsShortList, ' (', PI_Info.Year, ') N=',\n        COUNT(DISTINCT CONCAT(Measurements.SubjectNumber, Measurements.Ear))\n    ) AS Legend_Label\nFROM Measurements\n-- Join the PI_Info table to enrich the data with author and year information\nJOIN PI_Info \n-- Match rows based on the shared \"Identifier\" column    \n    ON Measurements.Identifier = PI_Info.Identifier \nWHERE Measurements.Identifier IN (\n    'Abur_2014', 'Feeney_2017', 'Groon_2015', 'Lewis_2015', \n    'Liu_2008', 'Rosowski_2012', 'Shahnaz_2006', 'Shaver_2013', \n    'Sun_2016', 'Voss_1994', 'Voss_2010', 'Werner_2010'\n)\nGROUP BY \n    Measurements.Identifier, \n    Measurements.Instrument, \n    PI_Info.AuthorsShortList, \n    Measurements.Frequency, \n    PI_Info.Year;\n\nData Visualization:\n\ndata$Frequency &lt;- as.numeric(data$Frequency)\n\n# Filter the data to include only rows where Frequency is 200 Hz or higher\ndata &lt;- data %&gt;%\n  filter(Frequency &gt;= 200)\n\n# Create the plot\nggplot(data, aes(x = Frequency, y = MeanAbsorbance, color = Legend_Label)) +\n# Adds lines representing Mean Absorbance for each Legend_Label group  \n  geom_line(size = 0.8) + \n  labs(\n    title = \"Mean absorbance from publications in WAI database\",\n    x = \"Frequency (Hz)\",\n    y = \"Mean Absorbance\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(\n# Transforms the x-axis to a logarithmic scale for better visualization of frequency values.    \n    trans = \"log10\",\n# Sets custom tick marks on the x-axis    \n    breaks = c(200, 400, 600, 800, 1000, 2000, 4000, 6000, 8000),\n# Labels for the tick marks\n    labels = c(\"200\", \"400\", \"600\", \"800\", \"1000\", \"2000\", \"4000\", \"6000\", \"8000\"),\n# Restricts the x-axis range to frequencies between 200 and 8000 Hz\n    limits = c(200, 8000)\n  ) +\n  scale_y_continuous(\n# Restricts the y-axis range to values between 0 and 1    \n    limits = c(0, 1),\n# Removes additional padding from the y-axis range    \n    expand = c(0, 0)\n  )\n\n\n\n\n\n\n\n\nAbove is the a replicate of Figure 1. Y-axis (Mean Absorbance) represents the average proportion of sound energy absorbed by the middle ear at various frequencies. Higher values indicate greater absorption, while lower values suggest more reflection. X-axis (Frequency in Hz) represents the sound frequency (logarithmic scale) ranging from 200 Hz to 8000 Hz.\nEach line corresponds to a specific study in the WAI database, identified by the first author’s name, publication year, and the number of participants.\nIn essence, the graph provides a comparative view of absorbance data from multiple studies. It highlights similarities and differences in how different populations and systems respond to sound frequencies.\nMost studies show increasing absorbance from 200 Hz to approximately 1000-2000 Hz, peaking, and then gradually decreasing at higher frequencies. Variability in the data such as differences in peak values and slopes likely reflects variations in study populations, equipment, or methodologies.\n\n\n\nI decided to choose the most recent study among the 12 selected publications, which was Feeney et al. (2017). This contains various grouping variables such as age, sex, and race/ethnicity. I chose Sex as the grouping variable because it is a common demographic factor in audiological studies and is likely to show differences in middle-ear characteristics.\nThe below SQL query takes on a similar process as that of recreating Figure 1. In this case, the Identifier would be the Feeney_2017, instead of all the 12 studies.\n\n\nSELECT \n    Measurements.Frequency, \n-- The sex of the subject (e.g., male, female)    \n    Subjects.Sex,\n    AVG(Measurements.Absorbance) AS MeanAbsorbance\nFROM Measurements\n-- Join the Subjects table to enrich data with subject information\nJOIN Subjects\n-- Match rows based on SubjectNumber (common key)\n    ON Measurements.SubjectNumber = Subjects.SubjectNumber\n-- Filter to include only data from the 'Feeney_2017' study\nWHERE Measurements.Identifier = 'Feeney_2017'\nGROUP BY \n    Measurements.Frequency, \n    Subjects.Sex\nORDER BY Measurements.Frequency;\n\nData Visualization\nI maintained a similar format for the x and y axis. The X-axis represents frequency (logarithmic scale), the Y-axis shows mean absorbance, and lines differentiate groups by sex (Male, Female, Unknown).\n\ndata$Frequency &lt;- as.numeric(data$Frequency)\n\nggplot(data, aes(x = Frequency, y = MeanAbsorbance, color = Sex)) +\n  geom_line(size = 0.8) +\n  labs(\n    title = \"Mean Absorbance by Sex Across Frequencies (Feeney et al., 2017)\",\n    x = \"Frequency (Hz)\",\n    y = \"Mean Absorbance\"\n  ) +\n  theme_minimal() +\n# Customize x and y axis  \n  scale_x_continuous(\n    trans = \"log10\",\n    breaks = c(200, 400, 600, 800, 1000, 2000, 4000, 6000, 8000),\n    labels = c(\"200\", \"400\", \"600\", \"800\", \"1000\", \"2000\", \"4000\", \"6000\", \"8000\"),\n    limits = c(200, 8000)\n  ) +\n  scale_y_continuous(\n    limits = c(0, 1),\n    expand = c(0, 0)\n  )\n\n\n\n\n\n\n\n\nThis graph illustrates how middle-ear absorbance varies across frequencies for male, female, and unknown sex groups in the Feeney et al. (2017) study. While the general trend of absorbance is consistent across groups, slight variations at the extremes of the frequency range (low and high) may warrant further exploration. The similarity across sexes suggests that sex is not a significant factor influencing WAI in this study.\nSome key observations:\n\nAbsorbance increases as frequency rises from 200 Hz to around 1000-2000 Hz, peaks, and then decreases for higher frequencies. All groups show peak absorbance in the range of 1000-4000 Hz, a typical finding in WAI studies, as this range represents optimal middle-ear energy absorption.\nThe absorbance patterns for male and female groups are very similar across frequencies, suggesting that sex may have minimal impact on WAI results in this study. The “Unknown” group also follows a similar trajectory, possibly due to overlapping populations.\nSlight differences can be seen at lower frequencies (200-400 Hz) and higher frequencies (&gt;6000 Hz), but these differences are marginal.\n\nBest practice purposes…\n\ndbDisconnect(con_wai)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Taehwan (Jack) Kim | 김태환",
    "section": "",
    "text": "My life so far…"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Taehwan (Jack) Kim | 김태환",
    "section": "Education",
    "text": "Education\nPomona College - Claremont, California, USA\n\nMajor in Economics and Minor in Data Science\nResident of Clark I\n\nChadwick International School - Incheon, South Korea\n\nFull IB Diploma\nHiger Levels: Language & Literature, Global Politics, History\nStandard Levels: Biology, Math, Spanish\n\nDankook University Middle School - Seoul, South Korea\n\n2 years of extremely rigorous grind at the hub of Korean education\nOnly-boys middle school\nEndless hours of private academies (hagwons)\n\nDaedo Elementary School - Seoul, South Korea\n\n5 years of extremely rigorous grind at the hub of Korean education\nEndless hours of private academies (hagwons)\n\nTransit Middle School - Buffalo, New York, USA\n\nWonderful break from Korean education\nA taste of cheerful life in US\n\nCountry Parkway Elementary School - Buffalo, New York, USA\n\nFreedom from Korean education in a peaceful town in Buffalo"
  },
  {
    "objectID": "about.html#work-experience",
    "href": "about.html#work-experience",
    "title": "Taehwan (Jack) Kim | 김태환",
    "section": "Work Experience",
    "text": "Work Experience\nKorea Search Fund\n\nCrucial part of a live deal\nWorking currently in Korean hours\n\nTortoise Capital Management\n\nDeveloped a passion for value investing\nBirth of a stock guru\n\nKim & Chang Law Firm\n\nExperienced the challenges of M&A deals\nImportance of connections in completing business deals\n\nDoOne Accounting Corporation\n\nEntire global financial system is powered by Excel 2021\nWork, Drink, Work more culture in Korea"
  },
  {
    "objectID": "about.html#activities-achievements",
    "href": "about.html#activities-achievements",
    "title": "Taehwan (Jack) Kim | 김태환",
    "section": "Activities & Achievements",
    "text": "Activities & Achievements\nStudent Unior for Reasonable Investment (SURI)\n\nExecutive Investment Manager of $1 million portfolio\n\nCertified Investment Manager\n\nCertified by Korean Financial Investment Association (KOFIA)"
  },
  {
    "objectID": "about.html#soccer-career",
    "href": "about.html#soccer-career",
    "title": "Taehwan (Jack) Kim | 김태환",
    "section": "Soccer Career",
    "text": "Soccer Career\nDaedo Elementary (2009-2013)\nSinjeong Elementary (2012)\nDakook Middle (2014-2015)\nChadwick International Varsity Soccer (2016-2020)\nSiheung FC U16, U17 (2015-2018)\nSongdo FC (2015-2020)\nPomona-Pitzer Varsity Soccer (2020-2022)"
  },
  {
    "objectID": "about.html#military-experience",
    "href": "about.html#military-experience",
    "title": "Taehwan (Jack) Kim | 김태환",
    "section": "Military Experience",
    "text": "Military Experience\nRepubic of Korea Army: Army Special Forces Reconnaissance Unit of 39th Infantry Brigade of 15th Infantry Division of Army 2nd Corps\n\nClassified missions"
  },
  {
    "objectID": "Project_5.html#project-1-updates",
    "href": "Project_5.html#project-1-updates",
    "title": "Project 5: Overview of DS2 - FA2025",
    "section": "Project 1 Updates",
    "text": "Project 1 Updates"
  },
  {
    "objectID": "Project_5.html#project-2-updates",
    "href": "Project_5.html#project-2-updates",
    "title": "Project 5: Overview of DS2 - FA2025",
    "section": "Project 2 Updates",
    "text": "Project 2 Updates\nfff"
  },
  {
    "objectID": "Project_5.html#project-3-updates",
    "href": "Project_5.html#project-3-updates",
    "title": "Project 5: Overview of DS2 - FA2025",
    "section": "Project 3 Updates",
    "text": "Project 3 Updates"
  },
  {
    "objectID": "Project_5.html#project-4-updates",
    "href": "Project_5.html#project-4-updates",
    "title": "Project 5: Overview of DS2 - FA2025",
    "section": "Project 4 Updates",
    "text": "Project 4 Updates"
  },
  {
    "objectID": "case_study_2.html#referee-decisions-for-arsenal-in-2021-2022-season",
    "href": "case_study_2.html#referee-decisions-for-arsenal-in-2021-2022-season",
    "title": "Arsenal in Premier League 2021-2022",
    "section": "",
    "text": "TidyTuesday Reference\nOriginal Data Source\nObjective: Which referees gave out the most yellow cards to Arsenal during the 2021-2022 season?\nThe dataset includes teams, referee, and stats by home and away side such as fouls, shots, cards, and many more. This dataset was compiled from the official Premier League website for the 2021-2022 season. This data was scrapped by Evan Gower.\n\nlibrary(tidytuesdayR)\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-04-04')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 14)\n\nsoccer &lt;- tuesdata$soccer\n\nOriginal Data\n\nlibrary(DT)\n\n# Display the first 5 rows of the soccer dataset\ndatatable(\n  soccer[1:5, ],  # Select the first 5 rows\n  options = list(\n    pageLength = 5,      # Show 5 rows\n    scrollX = TRUE,      # Enable horizontal scrolling\n    dom = 't',           # Show only the table without search box, etc.\n    autoWidth = TRUE     # Adjust column widths automatically\n  ),\n  caption = \"First 5 Rows of the Soccer Dataset\"\n)\n\n\n\n\n\nThe above sample table shows the first five rows of the original dataset. Each row represents a unique matchday for a given team. Descriptions of some key variables are the following:\n\nFTHG: full time home team goals\nFTAG: full time away team goals\nFTR: results showing which team won\nHalf-time results are denoted by HTHG, HTAG, HTR\nHS: Number of shots taken by the home team\nAS: Number of shots taken by the away team\nHST: Number of shots on target by the home team\nAST: Number of shots on target by the away team\nHF: Number of fouls by the home team\nAF: Number of fouls by the away team\nHC: Number of corners taken by the home team\nAC: Number of corners taken by the away team\nHY: Number of yellow cards received by the home team\nAY: Number of yellow cards received by the away team\nHR: Number of red cards received by the home team\nAR: Number of red cards received by the away team\n\nRefereeing decisions play huge outcome in a football match. In fact, fans and analysts alike do pay enormous attention to the designated referee for each fixture. As a lifelong Arsenal fan, I wanted to find out which referees tend to be harsher on Arsenal. More specifically, we focus on yellow cards. While a red card means that a player is ejected from the match, these instances tend to be more clear-cut decisions. Yellow card decisions offer a better insight into the deeper nuances of refereeing decisions or potential biases toward/against teams since yellow cards are given to less aggressive actions.\n\nlibrary(tidyverse)\n\n# Creating a comprehensive table for Disciplinary actions data\narsenal_yellow_by_referee &lt;- soccer |&gt;\n  filter(HomeTeam == \"Arsenal\" | AwayTeam == \"Arsenal\") |&gt;\n  group_by(Referee) |&gt;\n  summarise(\n    Home_Yellow_Cards = sum(HY[HomeTeam == \"Arsenal\"], na.rm = TRUE),\n    Away_Yellow_Cards = sum(AY[AwayTeam == \"Arsenal\"], na.rm = TRUE),\n    Total_Yellow_Cards = Home_Yellow_Cards + Away_Yellow_Cards\n  ) |&gt;\n  arrange(desc(Total_Yellow_Cards))\n\n\n\n\nlibrary(ggplot2)\n\n# Bar char to represent yellow cards by referees\nggplot(arsenal_yellow_by_referee, aes(x = reorder(Referee, -Total_Yellow_Cards), y = Total_Yellow_Cards)) +\n  geom_bar(stat = \"identity\", fill = \"yellow\") +\n  geom_text(aes(label = Total_Yellow_Cards), vjust = -0.5, size = 4) +\n  labs(title = \"Yellow Cards Arsenal Received by Referees (2021-2022 Season)\", \n       x = \"Referee\", \n       y = \"Total Yellow Cards\") +\n  scale_y_continuous(breaks = NULL) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1, size = 10), \n    axis.text.y = element_blank(),  \n    axis.ticks.y = element_blank(), \n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank()  \n  )\n\n\n\n\n\n\n\n\nThe bar chart represents the total number of yellow cards each referee has given to Arsenal. Craig Pawson handed out the most yellow cards to Arsenal, with 9 cards given.\n\n\n\n\nlibrary(ggplot2)\n\n# Summarize total yellow cards at home and away\nyellow_cards_summary &lt;- arsenal_yellow_by_referee %&gt;%\n  summarise(\n    Home = sum(Home_Yellow_Cards),\n    Away = sum(Away_Yellow_Cards)\n  ) |&gt;\n  pivot_longer(cols = everything(), names_to = \"Location\", values_to = \"Total_Yellow_Cards\")\n\n\nggplot(yellow_cards_summary, aes(x = Location, y = Total_Yellow_Cards, fill = Location)) +\n  geom_bar(stat = \"identity\", width = 0.6) +\n  geom_text(aes(label = Total_Yellow_Cards), vjust = -0.2, size = 5) + \n  labs(\n    title = \"Yellow Cards Arsenal Received by Location\",\n    x = \"Location\",\n    y = \"Total Yellow Cards\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.margin = margin(10, 10, 20, 10), \n    text = element_text(size = 14),\n    legend.position = \"none\"\n  ) +\n  ylim(0, max(yellow_cards_summary$Total_Yellow_Cards) + 5)\n\n\n\n\n\n\n\n\nThis study offers insight into if Arsenal experienced home advantage in referee decisions. Many fans assume that referees tend to be more lenient toward home teams. This may manifest the most in referee decisions for making disciplinary actions. The bar chart shows the number of yellow cards Arsenal received depending on match venue.\nInterestingly, Arsenal received 29 yellow cards at away matches and 31 in home matches. This goes against common logic. Although not conclusive, home advantage did not work in Arsenal’s favor."
  },
  {
    "objectID": "case_study_1.html#successful-countries-and-athletes-in-the-olympics",
    "href": "case_study_1.html#successful-countries-and-athletes-in-the-olympics",
    "title": "Success in Olympics",
    "section": "",
    "text": "TidyTuesday Reference\nOriginal Data Source\nObjective: Usually it is the Olympic athletes that receive the fame. However, at the end of the day they each represent their country with pride. Let’s find out which nations are represented the most in the top 10 most successful athlete list (measured by medal count). So, which countries have produced the best talents?\nThis analysis utilizes a public dataset on Kaggle compiled by RGriffen. This is a historical dataset on the modern Olympic Games, including all the Games from Athens 1896 to Rio 2016. It was compiled from scrapping data from Sport Reference on May 2018.\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-08-06')\n\nolympics &lt;- tuesdata$olympics\n\nOriginal Data\n\nlibrary(DT)\n\n# Display the first 5 rows as an interactive table\ndatatable(\n  olympics[1:5, ],  # Select the first 5 rows\n  options = list(\n    pageLength = 5,      # Show 5 rows\n    scrollX = TRUE,      # Enable horizontal scrolling\n    dom = 't',           # Show only the table (no search box, etc.)\n    autoWidth = TRUE     # Adjust column width automatically\n  ),\n  caption = \"First 5 Rows of the Olympics Dataset\"\n)\n\n\n\n\n\nAbove is a sample table showing the first 5 rows of Olympics dataset. This data includes all athletes who participated in past Olympics. Team represents the country each athlete represented and the variable “noc” represents the three-letter abbreviation. Some columns do have missing variable. For example, some athletes may have missing data on their height and weight. If the medal column is missing, it means that the player did not win a medal during his/her participation of that particular event. If a player did win a medal, it will be indicated as Bronze, Silver or Gold.\n\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# Treating Soviet Union and Russia as one country and representing it as USSR/Russia\nolympics &lt;- olympics |&gt;\n  mutate(team = case_when(\n    team %in% c(\"Soviet Union\", \"Russia\") ~ \"USSR/Russia\",\n    TRUE ~ team\n  ))\n\n#Obtaining best athletes\ntop_athletes &lt;- olympics |&gt;\n  filter(!is.na(medal)) |&gt;  # first filters for those only with medals\n  group_by(name, team) |&gt;\n  summarise(total_medals = n(), .groups = \"drop\") |&gt;\n  arrange(desc(total_medals)) |&gt;\n  slice(1:10)\n\n# Create a clean table for Top 10 Athletes\ntop_athletes |&gt;\n  kable(\n    caption = \"Top 10 Athletes by Total Medals (USSR/Russia Combined)\",\n    col.names = c(\"Name\", \"Team\", \"Total Medals\"),\n    align = \"lcl\"\n  ) |&gt;\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),\n    full_width = FALSE\n  )\n\n\nTop 10 Athletes by Total Medals (USSR/Russia Combined)\n\n\nName\nTeam\nTotal Medals\n\n\n\n\nMichael Fred Phelps, II\nUnited States\n28\n\n\nLarysa Semenivna Latynina (Diriy-)\nUSSR/Russia\n18\n\n\nNikolay Yefimovich Andrianov\nUSSR/Russia\n15\n\n\nBorys Anfiyanovych Shakhlin\nUSSR/Russia\n13\n\n\nEdoardo Mangiarotti\nItaly\n13\n\n\nOle Einar Bjrndalen\nNorway\n13\n\n\nTakashi Ono\nJapan\n13\n\n\nAleksey Yuryevich Nemov\nUSSR/Russia\n12\n\n\nDara Grace Torres (-Hoffman, -Minas)\nUnited States\n12\n\n\nJennifer Elisabeth \"Jenny\" Thompson (-Cumpelik)\nUnited States\n12\n\n\n\n\n\n\n# Filter by nations for the top athletes\ntop_nations &lt;- top_athletes |&gt;\n  group_by(team) |&gt;\n  summarise(count = n(), .groups = \"drop\") |&gt;\n  arrange(desc(count))\n\n# Create clean table for Top Nations\ntop_nations |&gt;\n  kable(\n    caption = \"Top Nations by Number of Top Athletes (USSR/Russia Combined)\",\n    col.names = c(\"Team\", \"Number of Athletes\"),\n    align = \"lc\"\n  ) |&gt;\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),\n    full_width = FALSE\n  )\n\n\nTop Nations by Number of Top Athletes (USSR/Russia Combined)\n\n\nTeam\nNumber of Athletes\n\n\n\n\nUSSR/Russia\n4\n\n\nUnited States\n3\n\n\nItaly\n1\n\n\nJapan\n1\n\n\nNorway\n1\n\n\n\n\n\n\n\nAs for most successful athletes, Michael Phelps by far has won the most medals (28). His success if unprecedented, with the second most decorated athlete having a whopping 10 fewer medals than Phelps.\n\nlibrary(ggplot2)\n\n# Creating a bar chart\nggplot(top_nations, aes(x = reorder(team, -count), y = count)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(title = \"Number of Top 10 Athletes by Country\",\n       x = \"Country\",\n       y = \"Number of Athletes\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nRussia (formerly USSR) produced the highest number of most successful athletes. Russia/USSR and the US usually rank top 2 for medal counts. It is actually surprising to find China not represented in the list, given the recent success of China in Olympics."
  },
  {
    "objectID": "case_study_3.html#some-trends-in-netflix-movies",
    "href": "case_study_3.html#some-trends-in-netflix-movies",
    "title": "Netflix Movies",
    "section": "",
    "text": "TidyTuesday Reference\nOriginal Data Source\nObjective: Analyze some interesting trends in Netflix movies as of 2019\nThis dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search engine.\nIn 2018, Flixable released an interesting report which shows that the number of TV shows on Netflix has nearly tripled since 2010. The streaming service’s number of movies has decreased by more than 2,000 titles since 2010, while its number of TV shows has nearly tripled. It will be interesting to explore what other insights can be obtained from the same dataset.\nIntegrating this dataset with other external datasets such as IMDB ratings, rotten tomatoes can also provide many interesting findings.\n\ntuesdata &lt;- tidytuesdayR::tt_load('2021-04-20')\ntuesdata &lt;- tidytuesdayR::tt_load(2021, week = 17)\n\nnetflix &lt;- tuesdata$netflix\n\nlibrary(tidyverse)\n\nOriginal Data\n\nlibrary(DT)\n\n# Display the first 5 rows of the netflix dataset\ndatatable(\n  netflix[1:5, ],  # Select the first 5 rows\n  options = list(\n    pageLength = 5,      # Show 5 rows\n    scrollX = TRUE,      # Enable horizontal scrolling\n    dom = 't',           # Show only the table without search box, etc.\n    autoWidth = TRUE     # Adjust column widths automatically\n  ),\n  caption = \"First 5 Rows of the Netflix Dataset\"\n)\n\n\n\n\n\nThe above sample table shows the first five rows of the original data. Descriptions of the variables are the following:\n\nshow_id: Unique ID for every Movie / Tv Show\ntype: Identifier - A Movie or TV Show\ntitle: Title of the Movie / Tv Show\ndirector: Director of the Movie/Show\ncast: Actors involved in the movie / show\ncountry: Country where the movie / show was produced\ndate_added: Date it was added on Netflix\nrelease_year: Actual Release year of the movie / show\nrating: TV Rating of the movie / show\nduration: Total Duration - in minutes or number of seasons\nlisted_in: Genre\ndescription: Summary description of the film/show\n\n\n\n\n# We want to look at actors for movies\nactor_count &lt;- netflix |&gt;\n  filter(type == \"Movie\", !is.na(cast)) |&gt; \n  separate_rows(cast, sep = \",\\\\s*\") |&gt; \n  count(cast, sort = TRUE)\n\n# Top 5 actors\ntop_5_actors &lt;- actor_count |&gt;\n  slice_head(n = 5) \n\n# Creating a bar chart but having the names on the vertical axis since they are long\nggplot(top_5_actors, aes(x = reorder(cast, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +  \n  labs(\n    title = \"Top 5 Actors Appearing in Netflix Titles\",\n    x = \"Actor\",\n    y = \"Number of Appearances\"\n  ) +\n  theme_minimal() \n\n\n\n\n\n\n\n\nInteresting, I actually do not know any of these actors/actresses. Upon some research, I have found that all of them are Indian actors/actresses. These actors/actresses were very active in their respective careers.\nAs a side analysis, let’s find out which countries produced the most number of movies?\n\n# Counting the movies by the countru they were produced\ncountry_count_movies &lt;- netflix |&gt;\n  filter(type == \"Movie\", !is.na(country)) |&gt;  \n  separate_rows(country, sep = \",\\\\s*\") |&gt;  \n  count(country, sort = TRUE)  \n\n# Finding the top 5 countries\ntop_5_countries &lt;- country_count_movies |&gt;\n  slice_head(n = 5)  \n\n\nggplot(top_5_countries, aes(x = reorder(country, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +  \n  labs(\n    title = \"Top 5 Countries Producing the Most Netflix Movies\",\n    x = \"Country\",\n    y = \"Number of Movies\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nUnsurprisingly, the US has produced the most Netflix movies. This makes sense given that Hollywood is the global hub of film production. The US has produced more than double the amount of movies than second place India and more than the movie productions of the next four countries combined. With the growing size of Bollywood, India comes in for second place. This may speak to the high number of Indian actors/actresses in the first study.\n\n\n\nI have always wondered which words are commonly used in movie titles. We have excluded articles, pronouns, prepositions and any common stop words.\n\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(stringr)\n\n# A list of words we want to exclude\nstop_words &lt;- c(\"a\", \"an\", \"the\", \"and\", \"in\",\n                \"on\", \"of\", \"with\", \"for\", \"to\",\n                \"he\", \"she\", \"my\", \"it\", \"him\",\n                \"her\", \"above\", \"below\", \"top\",\n                \"bottom\", \"between\", \"front\", \"back\",\n                \"beneath\", \"they\", \"me\", \"you\", \"them\",\n                \"us\", \"we\", \"that\", \"this\", \"these\",\n                \"those\", \"I\", \"from\", \"i\", \"at\", \"\")\n\n# Filter and preprocess the titles\nword_count &lt;- netflix |&gt;\n  subset(type == \"Movie\" & !is.na(title)) |&gt;\n  transform(clean_title = str_remove_all(title, \"[^A-Za-z\\\\s]\")) |&gt;\n  transform(clean_title = tolower(clean_title)) |&gt;\n  {\\(df) data.frame(clean_title = unlist(strsplit(df$clean_title, \"\\\\s+\")))}() |&gt;\n  subset(!clean_title %in% stop_words) |&gt;\n  table() |&gt;\n  as.data.frame()\n\n# Sort and rename columns\nword_count &lt;- word_count[order(-word_count$Freq), ]\ncolnames(word_count) &lt;- c(\"Word\", \"Frequency\")\n\n# Display top 10 words as a formatted table\nword_count |&gt;\n  head(10) |&gt;\n  kable(\n    caption = \"&lt;strong style='color:black; font-size:16px;'&gt;Top 10 Most Frequent Words in Netflix Movie Titles&lt;/strong&gt;\",\n    col.names = c(\"Word\", \"Frequency\"),\n    row.names = FALSE,  # Suppress row numbers\n    align = c(\"l\", \"c\"),\n    escape = FALSE\n  ) |&gt;\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),\n    full_width = FALSE,\n    position = \"center\"\n  )\n\n\nTop 10 Most Frequent Words in Netflix Movie Titles\n\n\nWord\nFrequency\n\n\n\n\nlove\n91\n\n\nchristmas\n72\n\n\nmovie\n60\n\n\nman\n58\n\n\nstory\n54\n\n\nlife\n44\n\n\nworld\n42\n\n\nlittle\n39\n\n\nlive\n38\n\n\none\n38\n\n\n\n\n\n\n\nLove and Christmas are the top two most common words. These results do makes sense as these are popular topics.\n\n\n\nThe most commercially successful filmmaker in the history of Hollywood is Steven Spielberg. Steven Spielberg is the creator of many of the most successful franchises and stand-alone films. They include the Indiana Jones series, the Jurassic Park series, Saving Private Ryan, and Jaws. Let’s figure out which of his movies are on Netflix.\n\n# Filter for Spielberg movies\nspielberg_movies &lt;- netflix |&gt;\n  filter(type == \"Movie\", str_detect(director, \"Steven Spielberg\"))\n\nspielberg_movies |&gt;\n  select(title) |&gt;  # Select only the title column\n  rename(\"Title\" = title) |&gt;  # Rename the column for clarity\n  kable(\n    caption = \"&lt;strong style='color:black; font-size:16px;'&gt;Steven Spielberg Movies Available on Netflix&lt;/strong&gt;\",\n    col.names = c(\"Title\"),\n    align = \"l\",\n    escape = FALSE \n  ) |&gt;\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),\n    full_width = FALSE,\n    position = \"center\"\n  )\n\n\nSteven Spielberg Movies Available on Netflix\n\n\nTitle\n\n\n\n\nCatch Me If You Can\n\n\nHook\n\n\nIndiana Jones and the Kingdom of the Crystal Skull\n\n\nIndiana Jones and the Last Crusade\n\n\nIndiana Jones and the Raiders of the Lost Ark\n\n\nIndiana Jones and the Temple of Doom\n\n\nLincoln\n\n\nSchindler's List\n\n\nThe Adventures of Tintin\n\n\nWar Horse\n\n\n\n\n\n\n\nSurprising, only 10 are available on Netflix. Given sheer amount of Spielberg movies we know of, this a very small number."
  },
  {
    "objectID": "case_study_3.html#too-lazy-for-theaters.-when-can-i-watch-a-new-movie-on-netflix",
    "href": "case_study_3.html#too-lazy-for-theaters.-when-can-i-watch-a-new-movie-on-netflix",
    "title": "Netflix Movies",
    "section": "4. Too lazy for theaters. When can I watch a new movie on Netflix?",
    "text": "4. Too lazy for theaters. When can I watch a new movie on Netflix?\nFollowing Covid-19, fewer people are going to the movies. At the same time over-the-top streaming services have become more influential. Nowadays, people are wondering “when is my favorite movie going to be on Netflix?”. So let’s find out, on average, how long does it take for a movie to be added on Netflix.\nTwo caveats: - We have exact dates for the when the movie was added, but only the year data is available for when it was released. Hence, we will just find the number of years. - Netflix started streaming services in 2007. Any movie produced before 2007 will be excluded because that just automatically increases the average.\n\nlibrary(lubridate)\nlibrary(knitr)\nlibrary(kableExtra)\n\nnetflix_movies &lt;- netflix |&gt;\n  subset(type == \"Movie\" & !is.na(date_added) & !is.na(release_year)) |&gt;\n  transform(\n    date_added = mdy(date_added),\n    release_year = as.numeric(release_year)\n  )\n\n# Extract the Year from the date_added Column\nnetflix_movies &lt;- netflix_movies |&gt;\n  transform(\n    added_year = year(date_added)\n  )\n\n# Calculate Time Difference in Years\nnetflix_movies &lt;- netflix_movies |&gt;\n  transform(\n    time_diff_years = added_year - release_year\n  )\n\n# Exclude movies produced before 2007\nnetflix_movies_filtered &lt;- netflix_movies |&gt;\n  subset(!is.na(time_diff_years) & time_diff_years &gt;= 0 & release_year &gt;= 2007)\n\naverage_time_diff &lt;- mean(netflix_movies_filtered$time_diff_years, na.rm = TRUE)\n\ndata.frame(`Average Time Difference (Years)` = round(average_time_diff, 2)) |&gt;\n  kable(\n    col.names = c(\"Average Time Difference (Years)\"),\n    align = \"c\"\n  ) |&gt;\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),\n    full_width = FALSE,\n    position = \"center\"\n  )\n\n\n\n\nAverage Time Difference (Years)\n\n\n\n\n2.46\n\n\n\n\n\n\n\nOn average, it takes 2.46 years for a movie to be added to Netflix after being released in theaters.\nAdditionally, let’s look at how the average time differ across content ratings. The motion picture content ratings used to indicate the suitability of a movie for different audiences. Below are the description of each rating retrieved from Spectrum.\n\nG: General audiences, suitable for all ages\nPG: Parental guidance suggested, some material may not be suitable for children\nPG-13: Parents strongly cautioned, some material may be inappropriate for children under 13\nR: Restricted, under 17 requires an accompanying parent or adult guardian\nNC-17: Clearly adult\nUR: Unrated\nNR: Not rated (not officially assigned)\nTV-Y: All children\nTV-Y7: Directed to older children\nTV-Y7-FV: Directed to older children - fantasy violence\nTV-G: General audience\nTV-PG: Parental guidance suggested\nTV-14: Parents strongly cautioned\nTV-MA: Mature audience only\n\n\nlibrary(lubridate)\nlibrary(ggplot2)\n\n\nmovies_data &lt;- netflix |&gt;\n  subset(type == \"Movie\" & !is.na(date_added) & !is.na(release_year)) |&gt;\n  transform(\n    date_added = mdy(date_added),\n    release_year = as.numeric(release_year)\n  )\n\n\nmovies_data &lt;- movies_data |&gt;\n  transform(\n    added_year = year(date_added)\n  )\n\n\nmovies_data &lt;- movies_data |&gt;\n  transform(\n    time_diff_years = added_year - release_year\n  )\n\n\nmovies_filtered &lt;- movies_data |&gt;\n  subset(!is.na(time_diff_years) & time_diff_years &gt;= 0 & release_year &gt;= 2007)\n\n\naverage_time_by_rating_table &lt;- aggregate(\n  time_diff_years ~ rating,\n  data = movies_filtered,\n  FUN = function(x) mean(x, na.rm = TRUE)\n)\n\n\naverage_time_by_rating_table &lt;- average_time_by_rating_table[order(-average_time_by_rating_table$time_diff_years), ]\n\n\nggplot(average_time_by_rating_table, aes(x = reorder(rating, -time_diff_years), y = time_diff_years, fill = rating)) +\n  geom_bar(stat = \"identity\", width = 0.7, show.legend = FALSE) +\n  geom_text(aes(label = round(time_diff_years, 2)), vjust = -0.5, size = 5) +\n  labs(\n    title = \"Average Time Difference by Rating (Movies Only)\",\n    x = \"Rating\",\n    y = \"Average Time Difference (Years)\"\n  ) +\n  theme_minimal() +\n  theme(\n    text = element_text(size = 14),\n    plot.title = element_text(hjust = 0.5),\n    axis.text.x = element_text(angle = 45, hjust = 1, size = 12),\n    plot.margin = margin(10, 10, 30, 10)\n  ) +\n  coord_cartesian(clip = \"off\")\n\n\n\n\n\n\n\n\nThis is a bar chart of the different averages computed for each movie rating type.\nUnrated movies (UR) exhibited the longest wait time for the movie to be uploaded on Netflix. This was followed by PG-13 rated movies and PG rated movies. UR, PG-13, PG, R, G, TV-14, and NR rated movies all took longer than the average time for all movies."
  },
  {
    "objectID": "Project_3.html#are-departure-delays-significantly-different-between-morning-and-evening-flights",
    "href": "Project_3.html#are-departure-delays-significantly-different-between-morning-and-evening-flights",
    "title": "NYC Flights Analysis",
    "section": "",
    "text": "Objective: In this analysis, I will investigate whether there is a significant difference in departure delays between morning and evening flights using data from the nycflights13 package. This data set contains all the relevant information of a particular flight schedule. To perform this analysis, I will perform a permutation test, where I will shuffle the time-of-day labels (morning/evening) and repeatedly calculate the difference in mean departure delays. This will help simulate the null hypothesis, which assumes that the time of day has no statistical significance on departure delays. By comparing the observed difference in delays to this simulated distribution, I will assess the statistical significance of the observed difference.\n[Null Hypothesis] There is no difference in the mean departure delays between morning flights and evening flights.\n\nlibrary(nycflights13)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(DT)\n\n\n# Create a new table from original to add a new column indicator morning or evening flight\nflights &lt;- nycflights13::flights |&gt;\n  filter(!is.na(dep_delay)) |&gt;\n  mutate(time_of_day = case_when(\n    hour &gt;= 5 & hour &lt; 11 ~ \"morning\",\n    hour &gt;= 17 & hour &lt; 23 ~ \"evening\",\n    TRUE ~ \"other\"\n  )) |&gt;\n  filter(time_of_day != \"other\")  # Exclude flights that are not in the morning or evening\n\n\n# Display the first 5 rows as an interactive table\ndatatable(\n  flights[1:5, ],  # Select the first 5 rows\n  options = list(\n    pageLength = 5,      # Display 5 rows per page\n    dom = 't',           # Show only the table (no search box, etc.)\n    scrollX = TRUE       # Enable horizontal scrolling\n  ),\n  caption = \"First 5 Rows of Flights Data\"\n)\n\n\n\n\n\nWe are using the nycflights13 data which contains the flight information in New York airports in 2013. The above sample table includes all the relevant information for each flight schedule and also has a new column called time_of_day as an indicator for whether the flight is in the morning or in the evening.\nMost importantly, the data shows the departure delay time (in minutes). A positive value means the flight was delayed. A negative value means the flight departed earlier than planned schedule. A value of zero indicates the flight departed on time. We will first define morning and evening flights. Morning flights are flights departing between 5:00am and 11:00am. Evening flights are flights departing from 5:00pm and 11:00pm (ex. First row, dep_time 517 means departure time is at 5:17am). We removed the flights that are not within these time ranges.\nLet’s first check how many morning and evening flights there are, respectively.\n\n# Count the number of flights for each time_of_day\nflight_counts &lt;- flights |&gt;\n  group_by(time_of_day) |&gt;\n  summarize(flight_count = n())\n\n# Create a bar chart\nggplot(flight_counts, aes(x = time_of_day, y = flight_count, fill = time_of_day)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = flight_count), vjust = -0.5) +\n  labs(\n    title = \"Number of Flights by Time of Day\",\n    x = \"Time of Day\",\n    y = \"Number of Flights\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nThere are more morning flights than evening flights. Moreover, the difference is noticeable, with about 20,000 more flights classified as morning.\nBelow we have the average delayed time for morning and evening flights.\n\nlibrary(ggplot2)\n\n# Calculate mean delays\nmean_delays &lt;- flights |&gt;\n  group_by(time_of_day) |&gt;\n  summarize(mean_delay = mean(dep_delay, na.rm = TRUE))\n\n# Create the bar chart\nggplot(mean_delays, aes(x = time_of_day, y = mean_delay, fill = time_of_day)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = round(mean_delay, 2)), vjust = -0.5) +\n  labs(\n    title = \"Mean Departure Delay by Time of Day\",\n    x = \"Time of Day\",\n    y = \"Mean Delay (minutes)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nThen we calculate the observed differences.\n\nobs_diff &lt;- flights |&gt;\n  group_by(time_of_day) |&gt;\n  summarize(mean_delay = mean(dep_delay)) |&gt;\n  summarize(diff = diff(mean_delay)) |&gt;\n  pull(diff)\n\n\n\nObserved Difference in Mean Delays (Morning - Evening): -19.23765 minutes\n\n\n[Permutation Test] We’ll shuffle the time labels (morning or evening) 1000 times and calculate the difference in mean delays for each permutation.\n\nset.seed(47)\n\nnull_dist &lt;- map_dbl(1:1000, ~ {\n  flights |&gt; \n    mutate(shuffled_time = sample(time_of_day)) |&gt; \n    group_by(shuffled_time) |&gt; \n    summarize(mean_delay = mean(dep_delay), .groups = \"drop\") |&gt; \n    summarize(diff = diff(mean_delay)) |&gt; \n    pull(diff)\n})\n\nLet’s plot the null distribution with observed difference\n\nggplot(data.frame(null_dist), aes(x = null_dist)) +\n  geom_histogram(binwidth = 0.5, fill = \"lightblue\", color = \"black\") +\n  geom_vline(xintercept = obs_diff, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Permutation Test: Morning vs Evening Delays\",\n    x = \"Difference in Mean Delays\",\n    y = \"Frequency\"\n  )\n\n\n\n\n\n\n\n\n\n# Calculate p-value\np_value &lt;- mean(abs(null_dist) &gt;= abs(obs_diff))\n\n\n\nP-value: 0 \n\n\nResults\nThe red dashed line represents the observed difference in mean delays between morning and evening flights. The histogram represents the differences in mean delays generated under the null hypothesis (shuffling the time labels).\nA p-value of 0 means that none of the 1000 permutations generated a difference in mean delays as extreme (or more extreme) as the observed difference. This indicates that the observed difference is highly unlikely to have occurred by chance if the null hypothesis were true.\nSince the p-value is 0, this provides strong evidence against the null hypothesis. You would reject the null hypothesis and conclude that the time of day (morning vs. evening) may be correlated to departure delays.\nThe visualization supports this as the red line falls outside the range of the simulated null distribution values, highlighting that the observed difference is not consistent with the null hypothesis assumption.\nGreater Insights\nThis study provides strong evidence for the correlation between time of day and average departure delay time. According to the analysis, evening flights have nearly a 20 minute longer delays than morning flights. This is interesting given the fact that there were significantly more morning flights than evening flights. While more number of flights does not automatically indicate more flight traffic, the results are interesting.\nJust to dive, some potential reasons could explain the longer delays for evening flights. Below are some factors to consider:\n\nCumulative Delays Throughout the Day: Flights in the evening are often impacted by delays that build up over the day (e.g., late arrivals, extended turnaround times).\nWeather Conditions: Thunderstorms and other adverse weather conditions are more common in the late afternoon or early evening, especially in summer months.\nAir Traffic Congestion: Evening hours are often a peak time for flights returning from major business or leisure destinations, leading to increased congestion in the air and at the airport.\nCrew and Aircraft Rotation: If an aircraft or crew is late arriving from an earlier flight, it can cascade into delays for subsequent evening flights.\nRunway Closures or Maintenance: Some airports schedule runway or terminal maintenance for late evenings, which can cause operational disruptions.\n\nTo find some empirical reasons directly from the data, let’s look at early departure flights (those that have negative values for dep_delay column). The observed means of departure delays that we calculated in the above analysis takes into account all flights, regardless of whether their status was on time, early departure, or delayed departure. If either morning or evening flights have a significant number of early flights, then, that may potentially have affected the averages.\n\n# Filter flights with dep_delay &lt; 0 and group by time_of_day\nearly_departures &lt;- flights |&gt;\n  filter(dep_delay &lt; 0) |&gt;\n  group_by(time_of_day) |&gt;\n  summarize(early_flights = n())\n\n# Create a bar chart with adjusted limits and label position\nggplot(early_departures, aes(x = time_of_day, y = early_flights, fill = time_of_day)) +\n  geom_bar(stat = \"identity\", width = 0.6) +\n  geom_text(aes(label = early_flights), vjust = -0.2, size = 5) +\n  labs(\n    title = \"Number of Flights Departing Earlier Than Scheduled by Time of Day\",\n    x = \"Time of Day\",\n    y = \"Number of Early Departures\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n    axis.title.x = element_text(size = 12),\n    axis.title.y = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  ) +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.05)))\n\n\n\n\n\n\n\n\nLooking at the results, there are significantly more morning flights that departed earlier than scheduled. This has contributed to the significantly lower average departure delay for morning flights. This offers some empirical evidence based on our database as to why our calculated averages have a significant difference.\nWhile this particular caveat lends way to the argument for calculating the departure delay averages for those flights that were actually delayed, I believe this misleads the purpose of the study. You do not know whether your flight is going to be departing earlier than planned or delayed. We all expect our flights to depart on time. Therefore, simply calculating the averages for those flights that do actually get delayed misses this initial thought process. One particular flight can be delayed for some reason A. Well, that same reason A can be the basis for an early flight departure of another flight. Hence, I still believe it is more appropriate to examine all flights.\nSo how can we interpret these results in a broader context? The current analysis is based on flights departing from NYC airports (JFK, LGA, EWR) in 2013 and data acquired under the assumption that morning and evening flights do not have missing delay information. This means the conclusion of a significant difference in mean delays applies only to NYC flights in 2013.\nFor potential larger populations, we could generalize the results to other years in NYC. Similar patterns of delays (morning flights being less delayed than evening flights) might hold for other years if no major changes occurred in scheduling, infrastructure, or air traffic. It may also be possible to generalize to similar metropolitan hubs. NYC is one of the busiest air travel hubs in the US, so patterns observed in NYC might apply to other large cities with comparable air traffic and flight schedules, such as Chicago (ORD, MDW), Los Angeles (LAX), or Atlanta (ATL). However, generalization should be done cautiously and supported by additional evidence. Moreover, there may be limitations to generalization in the sense that regional factors may come into play. NYC has unique air traffic patterns due to its size, location, and volume of international and domestic flights. Other cities may not share these characteristics."
  },
  {
    "objectID": "Project_4.html#wideband-acoustic-immittance-wai-analysis",
    "href": "Project_4.html#wideband-acoustic-immittance-wai-analysis",
    "title": "Wideband Acoustic Immittance Analysis using SQL",
    "section": "",
    "text": "Objective: Replicating WAI Analysis and Examine a Single Study\nOriginal Data Source\nBackground: The WAI database is a comprehensive online database for normative adult WAI measurements. This database is designed to facilitate data sharing and analysis among researchers in the field of audiology. As of July 1, 2019, the database encompasses measurements from 12 peer-reviewed studies, totaling 640 subjects and 914 normal middle ears. This results in 286,774 data points across various frequencies. The establishment of this WAI database represents a significant advancement in audiological research, offering a centralized resource for normative data and promoting collaborative efforts in the study of middle-ear pathologies.\n\n\nThe objective is to recreate Figure 1, which represents mean absorbance data from select 12 publications in the Wideband Acoustic Immittance (WAI) database, showing how absorbance varies with frequency across different studies.\nStarter Code to establish connection to the WAI database.\n\nlibrary(DBI)\nlibrary(RMariaDB)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ncon_wai &lt;- dbConnect(\n  MariaDB(),\n  host = \"scidb.smith.edu\",\n  user = \"waiuser\",\n  password = \"smith_waiDB\",\n  dbname = \"wai\"\n)\n\nMeasurements &lt;- tbl(con_wai, \"Measurements\")\nPI_Info &lt;- tbl(con_wai, \"PI_Info\")\nSubjects &lt;- tbl(con_wai, \"Subjects\")\n\nThis SQL code below is for processing and aggregating data from the WAI database. More specifically, this combines data from two tables, Measurements and PI_Info, to calculate and aggregate absorbance values for specific studies and prepare data for visualization.\nThe following is the detailed process:\n\nCalculate Mean Absorbance: for each unique combination of study, instrument, frequency, and year.\nGenerates Legend Labels: constructs a label for visualization\nFilters Data: restricts the query to 12 specific studies of interest.\nGroups Data: organizes results by study, frequency, instrument, and year.\n\n\n-- Select the relevant columns and compute aggregated values\nSELECT \n    Measurements.Identifier, \n    PI_Info.AuthorsShortList, \n    Measurements.Instrument, \n    Measurements.Frequency, \n    AVG(Measurements.Absorbance) AS MeanAbsorbance, \n-- Create a descriptive label combining author names, year, and sample size    \n    CONCAT(\n        PI_Info.AuthorsShortList, ' (', PI_Info.Year, ') N=',\n        COUNT(DISTINCT CONCAT(Measurements.SubjectNumber, Measurements.Ear))\n    ) AS Legend_Label\nFROM Measurements\n-- Join the PI_Info table to enrich the data with author and year information\nJOIN PI_Info \n-- Match rows based on the shared \"Identifier\" column    \n    ON Measurements.Identifier = PI_Info.Identifier \nWHERE Measurements.Identifier IN (\n    'Abur_2014', 'Feeney_2017', 'Groon_2015', 'Lewis_2015', \n    'Liu_2008', 'Rosowski_2012', 'Shahnaz_2006', 'Shaver_2013', \n    'Sun_2016', 'Voss_1994', 'Voss_2010', 'Werner_2010'\n)\nGROUP BY \n    Measurements.Identifier, \n    Measurements.Instrument, \n    PI_Info.AuthorsShortList, \n    Measurements.Frequency, \n    PI_Info.Year;\n\nData Visualization:\n\ndata$Frequency &lt;- as.numeric(data$Frequency)\n\n# Filter the data to include only rows where Frequency is 200 Hz or higher\ndata &lt;- data %&gt;%\n  filter(Frequency &gt;= 200)\n\n# Create the plot\nggplot(data, aes(x = Frequency, y = MeanAbsorbance, color = Legend_Label)) +\n# Adds lines representing Mean Absorbance for each Legend_Label group  \n  geom_line(size = 0.8) + \n  labs(\n    title = \"Mean absorbance from publications in WAI database\",\n    x = \"Frequency (Hz)\",\n    y = \"Mean Absorbance\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(\n# Transforms the x-axis to a logarithmic scale for better visualization of frequency values.    \n    trans = \"log10\",\n# Sets custom tick marks on the x-axis    \n    breaks = c(200, 400, 600, 800, 1000, 2000, 4000, 6000, 8000),\n# Labels for the tick marks\n    labels = c(\"200\", \"400\", \"600\", \"800\", \"1000\", \"2000\", \"4000\", \"6000\", \"8000\"),\n# Restricts the x-axis range to frequencies between 200 and 8000 Hz\n    limits = c(200, 8000)\n  ) +\n  scale_y_continuous(\n# Restricts the y-axis range to values between 0 and 1    \n    limits = c(0, 1),\n# Removes additional padding from the y-axis range    \n    expand = c(0, 0)\n  )\n\n\n\n\n\n\n\n\nAbove is the a replicate of Figure 1. Y-axis (Mean Absorbance) represents the average proportion of sound energy absorbed by the middle ear at various frequencies. Higher values indicate greater absorption, while lower values suggest more reflection. X-axis (Frequency in Hz) represents the sound frequency (logarithmic scale) ranging from 200 Hz to 8000 Hz.\nEach line corresponds to a specific study in the WAI database, identified by the first author’s name, publication year, and the number of participants.\nIn essence, the graph provides a comparative view of absorbance data from multiple studies. It highlights similarities and differences in how different populations and systems respond to sound frequencies.\nMost studies show increasing absorbance from 200 Hz to approximately 1000-2000 Hz, peaking, and then gradually decreasing at higher frequencies. Variability in the data such as differences in peak values and slopes likely reflects variations in study populations, equipment, or methodologies.\n\n\n\nI decided to choose the most recent study among the 12 selected publications, which was Feeney et al. (2017). This contains various grouping variables such as age, sex, and race/ethnicity. I chose Sex as the grouping variable because it is a common demographic factor in audiological studies and is likely to show differences in middle-ear characteristics.\nThe below SQL query takes on a similar process as that of recreating Figure 1. In this case, the Identifier would be the Feeney_2017, instead of all the 12 studies.\n\n\nSELECT \n    Measurements.Frequency, \n-- The sex of the subject (e.g., male, female)    \n    Subjects.Sex,\n    AVG(Measurements.Absorbance) AS MeanAbsorbance\nFROM Measurements\n-- Join the Subjects table to enrich data with subject information\nJOIN Subjects\n-- Match rows based on SubjectNumber (common key)\n    ON Measurements.SubjectNumber = Subjects.SubjectNumber\n-- Filter to include only data from the 'Feeney_2017' study\nWHERE Measurements.Identifier = 'Feeney_2017'\nGROUP BY \n    Measurements.Frequency, \n    Subjects.Sex\nORDER BY Measurements.Frequency;\n\nData Visualization\nI maintained a similar format for the x and y axis. The X-axis represents frequency (logarithmic scale), the Y-axis shows mean absorbance, and lines differentiate groups by sex (Male, Female, Unknown).\n\ndata$Frequency &lt;- as.numeric(data$Frequency)\n\nggplot(data, aes(x = Frequency, y = MeanAbsorbance, color = Sex)) +\n  geom_line(size = 0.8) +\n  labs(\n    title = \"Mean Absorbance by Sex Across Frequencies (Feeney et al., 2017)\",\n    x = \"Frequency (Hz)\",\n    y = \"Mean Absorbance\"\n  ) +\n  theme_minimal() +\n# Customize x and y axis  \n  scale_x_continuous(\n    trans = \"log10\",\n    breaks = c(200, 400, 600, 800, 1000, 2000, 4000, 6000, 8000),\n    labels = c(\"200\", \"400\", \"600\", \"800\", \"1000\", \"2000\", \"4000\", \"6000\", \"8000\"),\n    limits = c(200, 8000)\n  ) +\n  scale_y_continuous(\n    limits = c(0, 1),\n    expand = c(0, 0)\n  )\n\n\n\n\n\n\n\n\nThis graph illustrates how middle-ear absorbance varies across frequencies for male, female, and unknown sex groups in the Feeney et al. (2017) study. While the general trend of absorbance is consistent across groups, slight variations at the extremes of the frequency range (low and high) may warrant further exploration. The similarity across sexes suggests that sex is not a significant factor influencing WAI in this study.\nSome key observations:\n\nAbsorbance increases as frequency rises from 200 Hz to around 1000-2000 Hz, peaks, and then decreases for higher frequencies. All groups show peak absorbance in the range of 1000-4000 Hz, a typical finding in WAI studies, as this range represents optimal middle-ear energy absorption.\nThe absorbance patterns for male and female groups are very similar across frequencies, suggesting that sex may have minimal impact on WAI results in this study. The “Unknown” group also follows a similar trajectory, possibly due to overlapping populations.\nSlight differences can be seen at lower frequencies (200-400 Hz) and higher frequencies (&gt;6000 Hz), but these differences are marginal.\n\nBest practice purposes…\n\ndbDisconnect(con_wai)"
  },
  {
    "objectID": "Project_5.html",
    "href": "Project_5.html",
    "title": "Project 5: Overview of DS2 - FA2025",
    "section": "",
    "text": "Analyze some interesting trends in Netflix movies as of 2019\nVery interesting tidytuesday dataset on Netflix movies and TV series."
  },
  {
    "objectID": "Project_5.html#recap-of-project-2",
    "href": "Project_5.html#recap-of-project-2",
    "title": "Project 5: Overview of DS2 - FA2025",
    "section": "",
    "text": "Analyze some interesting trends in Netflix movies as of 2019\nVery interesting tidytuesday dataset on Netflix movies and TV series."
  },
  {
    "objectID": "Project_5.html#summary-of-project-2",
    "href": "Project_5.html#summary-of-project-2",
    "title": "Project 5: Overview of DS2 - FA2025",
    "section": "Summary of Project 2",
    "text": "Summary of Project 2\nAnalyze some interesting trends in Netflix movies as of 2019\nVery interesting tidytuesday dataset on Netflix movies and TV series."
  },
  {
    "objectID": "Project_5.html#overview-of-project",
    "href": "Project_5.html#overview-of-project",
    "title": "Flight Delays",
    "section": "Overview of Project",
    "text": "Overview of Project\nAre departure delays significantly different between morning and evening flights?\n\nFlight delays are vey common"
  },
  {
    "objectID": "Project_5.html#most-popular-words-in-movie-titles",
    "href": "Project_5.html#most-popular-words-in-movie-titles",
    "title": "Movies on Netflix",
    "section": "Most Popular Words in Movie Titles",
    "text": "Most Popular Words in Movie Titles"
  },
  {
    "objectID": "Project_5.html#when-is-it-available-on-netflix",
    "href": "Project_5.html#when-is-it-available-on-netflix",
    "title": "Movies on Netflix",
    "section": "When is it available on Netflix?",
    "text": "When is it available on Netflix?"
  },
  {
    "objectID": "Project_5.html#nycflight2013-data",
    "href": "Project_5.html#nycflight2013-data",
    "title": "Flight Delays",
    "section": "nycflight2013 Data",
    "text": "nycflight2013 Data\n\n\n\n\n\n\nMornings: 5:00am ~ 11:00am\nEvenings: 5:00pm ~ 11:00pm"
  },
  {
    "objectID": "Project_5.html#observed-difference-in-mean-delay",
    "href": "Project_5.html#observed-difference-in-mean-delay",
    "title": "Flight Delays",
    "section": "Observed Difference in Mean Delay",
    "text": "Observed Difference in Mean Delay\n\n\n\nMean Departure Delay by Time of Day\n\n\nTime of Day\nMean Delay (minutes)\n\n\n\n\nevening\n22.729753\n\n\nmorning\n3.492098\n\n\n\n\n\n\n\n\n\nObserved Difference in Mean Delays (Morning - Evening): -19.23765 minutes"
  },
  {
    "objectID": "Project_5.html#permutation-test-and-results",
    "href": "Project_5.html#permutation-test-and-results",
    "title": "Flight Delays",
    "section": "Permutation Test and Results",
    "text": "Permutation Test and Results\n\n\n\nP-value: 0"
  },
  {
    "objectID": "Project_5.html#conclusion",
    "href": "Project_5.html#conclusion",
    "title": "Flight Delays",
    "section": "Conclusion",
    "text": "Conclusion\nObserved difference is highly unlikely to have occurred by chance if the null hypothesis were true.\nStrong evidence to reject null hypothesis: red line falls outside the range of the simulated null distribution values.\nPotential generalization into other mega airports in US:\n\nChicago (ORD, MDW)\nLos Angeles (LAX)\nAtlanta (ATL)"
  }
]